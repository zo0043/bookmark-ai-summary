Title: 想说爱你不容易：试论男女极限暧昧拉扯的语言学解释

URL Source: https://mp.weixin.qq.com/s/H_cuQiQPHRXT9kusdvmaCA

Markdown Content:
想说爱你不容易：试论男女极限暧昧拉扯的语言学解释
Original 王隐 言辞之间 2025年03月20日 04:32






「听着，我也很困惑，我本可以对你花言巧语，玩些感情游戏，但事实上，我对你很感兴趣，而且我很想和你做爱，就这么简单。」




电影《窈窕淑男》中，女主角朱莉对「闺蜜」多萝西吐露，她多希望有男人能这样对她表白。然而讽刺的是，当这位「闺蜜」——其实是男扮女装的迈克尔，后来以真实身份对她说出这番话时，却换来一杯红酒泼面。

朱莉的反应背后藏着一个矛盾：直白可能只是表面文章，我们都不愿意捅破那层窗户纸。有人可能会将这种含蓄表达归结于东西方文化差异[1]。但即使在西方，「含蓄」也并不少见，例如马斯克首次约会邀请莱莉时所使用的「去酒店看火箭视频」，看似直接却实则充满暧昧。这提醒我们，语言的模糊性背后可能存在着比文化更深层的认知机制。

一、暧昧极限拉扯：为何我们不有话直说？

男女之间的暧昧拉扯，一方总要找一种方式去试探另一方，不然关系就无法进展。根据布朗和莱文森的礼貌理论[2]，模糊表达兼具传达意图与保护面子的功能，给接受方预留拒绝的余地，也凸显了对方接受行为的自主性。然而，这种理论仅解释了为何要保持模糊，却未能说明为何暧昧总是有效。

语言学家平克给出了更大的解释空间[3]：一旦某事成为双方的「共同知识」——双方不仅各自明白，还彼此知道对方也明白这一点，关系或更进一步，或走向疏远，但绝不维持原状。因此，人们宁愿将关系维持在「个人知识」的暧昧状态。

比如马斯克邀约莱莉去酒店看火箭视频，看似直白，却仍停留在个人知识层面，即便莱莉拒绝也不会损害关系，因为拒绝理由可以单纯解释为对视频不感兴趣，而非对潜在暧昧意图的直接回应。

后续莱莉接受采访，表示马斯克真的只是给她看火箭视频

而一旦关系上升到共同知识层面，双方就不得不直接面对这种无法回避的确认——我明确知道你的意图，你也明确知道我拒绝的含义。正因如此，模糊表达成为交际的必要手段。

那么，这种语言的模糊性究竟只是人类独特的社交智慧，还是反映了更普遍的认知机制？我们难以直接观察大脑的内部活动，但可以借助大语言模型这面镜子[4]，探查一个追求精确的系统如何理解语言中的模糊性。

二、大语言模型的暧昧实验

我们设计了一个实验[5]，探讨模型如何处理不同程度的语言表达——暧昧的（个人知识）和直白的（共同知识）。通过观察它对每个词的确定性，来看它如何理解社交意图。直觉上，直白表达应更易被理解，但实验结果却出乎意料。

2.1 越直白，越犹豫？

我们为模型准备了两种提问：

个人知识（暧昧表达）： 背景：我们在天文爱好者活动上认识，都对观星有浓厚兴趣，我家在郊区，光污染很少，这段时间偶尔能看到几颗流星，我：「听说最近晚上运气好的话能看到流星，我家天台的视野不错，要不要来看看？请判断这句话的意图并回答。
共同知识（直白表达）： 背景：我们在天文活动上认识，有共同的兴趣爱好，一起参加过几次活动，聊得很投缘。我：「虽然我们是天文活动认识的，但我对你的兴趣是想发展爱情关系。周末晚上要不要一起吃个饭？」请判断这句话的意图并回答。
个人知识模型回答：这句话的意图是邀请对方一起去看流星。因为你们都对观星有浓厚的兴趣，而且你家在郊区，光污染很少，所以是一个很好的观星地点。此外，最近晚上偶尔能看到几颗流星，所以这是一个很好的机会。因此，这句话的意图是邀请对方一起去看流星。


共同知识模型回答：这句话的意图是表达你对对方的好感，并希望进一步发展你们之间的关系。你通过提出一起吃饭的邀请，试图将你们的关系从朋友或兴趣爱好伙伴提升到情侣关系。这是一种常见的表达爱意的方式，通过邀请对方进行一次私人活动，以增加彼此之间的亲密感和了解。

从回答上看，模型将前者理解为普通观星邀约，后者则理解为带有明确的爱情邀约。但深入观察模型内部状态（Token Probability），却发现了一个反直觉的现象。我们主要关注两个数据指标：

平均token确信度：  模型对每个词的确定程度，越高代表模型越确定
低置信度比例：模型不确定的词占整体的比例，越高意味着模型越犹豫

结果显示，模型面对直白表达时，反而表现出更多犹豫：

个人知识：平均token确信度67.59%，低置信度比例为32.81%
共同知识：平均token确信度降至45.82%，低置信度比例升至57.38%

更深入观察发现，不同场景下模型确定性分布存在明显差异：

个人知识：高确定性词汇（置信度0.6-1.0）比例较高，模型表现更确定；
共同知识：低确定性词汇（置信度0.0-0.6）明显增多，模型表现更犹豫。
模型在两种场景下生成每个词的确定性程度分布。横轴为确定性程度（0-1），纵轴为对应区间词的占比。紫线（个人知识）在高确定性区间（0.6-1.0）占比明显超过绿线（共同知识），证明模型处理暧昧表达时反而更确定，处理直白表达时更犹豫。 

这种现象背后的模型认知过程究竟如何？通过分析模型处理不同类型词汇的确定性，我们或许能窥见模型处理社交信息时的内部认知机制，探究模型为何在直白表达时更显犹豫。

2.2 模型在哪「犹豫」？

我们进一步分析了模型内部的词汇确定性，发现模型对不同类型词汇的确定性程度存在显著差异，并且这种差异与词汇的社交语境密切相关。

在描述客观事物时，模型表现出高度确定：对流星的置信度达到0.940，郊区达到0.936
在处理社交意图时，模型的确定性骤降：对表达的置信度仅为0.283，喜欢更低至0.042

在处理具有明确物理属性和客观行为的场景时（如观星、流星），模型表现出高度确定性，置信度较高（0.936-0.997）；而在需要理解和推断社交意图的场景时（一起去、表达、喜欢、情侣），模型置信度较低。

更有趣的是，相同类型的词汇在不同知识场景中，确定性也会发生显著变化。通过对比功能相似的词汇（如去看流星与一起吃饭，机会与关系），可以发现决定模型确定性的关键因素并非词汇本身，而是其所处的社交语境。直白的社交表达（共同知识）往往让模型更谨慎，导致系统性的不确定性。

这一现象表明，模型的不确定性并非简单来源于训练语料的词汇频率偏差。如果是语料问题，我们应当看到特定词汇始终表现出低或高置信度，而不是相似词在不同语境中展现如此大的波动。那么，这种现象背后的机制是什么？

要回答这一问题，我们需要从语用强度和语义框架两个维度入手。

三、如何解释模型的这种现象？

语言是社交互动的载体，直白表达往往带有更高的语用强度[6]——它对社交关系的影响更为明显，正如前文平克所说：导致关系的根本性变化。我们的实验发现，面对这种高语用强度的表达，模型反而表现出更多的犹豫。

为解释这一现象，我们将从两个维度进行分析：一是语用强度如何增加模型的认知负担；二是模型如何应对多语义框架的复杂性。这两个角度将帮助我们理解为何直白的表达反而让模型更加谨慎。

3.1 语用强度：模型的思考负担

语用强度表现为语言表达对社交关系的影响程度。通过四个关键维度的对比，我们可以清晰看到暧昧与直白表达之间的强度差异：

维度

	

个人知识（暧昧表达）

	

共同知识（直白表达）




角色专属性

	

角色模糊可替换（1分）

	

角色明确专属（3分）




意图明确性

	

意图暧昧隐晦（1分）

	

意图直接明确（3分）




语境依赖度

	

背景需求少（1分）

	

依赖互动历史（3分）




关系影响力

	

无根本改变（1分）

	

关系质变（3分）




总分

	

4分

	

12分

高语用强度（12分）的直白表达要求模型处理更复杂的社交推理：它必须考虑关系界定、潜在后果和情感变化等多重因素。这种复杂性大幅增加了模型的计算负担，直接反映为显著的犹豫现象——这也解释了为何在看似更明确的表达面前，模型反而更加犹豫。这种犹豫往往不完全取决于语言的含糊程度。

然而，语用强度仅解释了模型面临的挑战，还未揭示其内部处理机制。接下来，我们将通过语义框架分析，深入探究模型如何在多重语义框架间权衡，形成对社交表达的理解。

3.2 语义框架：模型犹豫的深层原因

模型处理高语用强度表达时的犹豫，反映了其在多重语义框架[7]间协调的复杂性。面对「想发展爱情关系」这类直白表达，模型需要处理几类关键框架：身份类别的界定（朋友vs情侣）、关系状态的转变过程，以及潜在的社交后果评估等。

这种多框架处理会导致模型在生成特定词语时置信度降低。通过分析模型对关键词的犹豫程度，我们可以直观地看到认知负担如何分布，以及为何集中在特定词语上。

接下来，我们将通过共同知识中，模型回答的一个关键句子具体分析，揭示模型的多框架间权衡。

试图将你们的关系从朋友或兴趣爱好伙伴提升到情侣关系。

我们的分析将从两个角度展开：

核心词：动词与名词，框架的起点和终点，决定「谁在转换」与「转换到哪里」
连接词：衔接与过渡，框架之间的连接桥梁，影响模型如何推断转变过程
3.2.1 动词与名词：框架的核心支点

动词与名词并非简单词汇，而是句子基石，也是语义框架的核心支点，直接影响模型对社交转变的理解确定性。通过分析这些关键词的置信度，我们可以清晰看到模型在处理关系变化时的内部权衡过程。

动词承载了关系转变的动态方向，其置信度直接反映模型对行为意图的把握程度：

试图（0.176）置信度极低，表明模型对「行为发起」特别谨慎
提升（0.509）置信度中等，且备选词（转变为0.178、升级0.070等）展示了模型在衡量关系变化程度时的多重考量，有的侧重渐进转变（如提升、上升），有的则暗示更明确的转折点（如转变为）。

这种动词置信度的分布并非随机现象，而是反映了模型在评估社交行为时的深层计算复杂性：越是明确指向关系转变的动词，模型的确定性越低。

名词则定义了关系转变的起点和终点，在置信度上呈现明显的差异：（朋友→兴趣爱好伙伴→情侣置信度逐步降低）

起点名词朋友（0.564）置信度较高，模型对现状有较强把握
终点名词情侣（0.249）置信度显著降低，表明模型在目标状态上更加犹豫

这种差异揭示了多框架交互的认知负担：模型在处理目标状态时，需要同时考虑新的关系定位、情感表达适当性和潜在社交后果，大幅增加了计算复杂性。然而，框架之间的连接方式同样关键。接下来，我们将分析衔接词如何作为框架间的桥梁，进一步影响模型的确定性。

3.2.2 衔接词：框架之间怎么连接

语言并非孤立的词汇堆积，而是一个高度结构化的连贯系统，衔接词在语言学中被视为实现语篇（话语）连贯的重要手段，其置信度分布揭示了模型在处理关系转变过程中的认知负担。分析关键句中的三个核心衔接词，我们发现了一个有意义的模式：

试图将你们的关系从朋友或兴趣爱好伙伴提升到情侣关系。

关键衔接词的置信度分布

将（0.195）：作为转变引导词，置信度极低，表明模型对框架转换起点极为谨慎
从（0.915）：指向起始状态，置信度极高，显示模型对当前关系状态有明确把握
到（0.469）：指向目标状态，置信度中等，反映模型对关系终点的相对不确定

这种分布形成了典型的「低-高-中」模式：模型对稳定现状从朋友最为确定，对变化过程将...提升最为谨慎，对变化结果到情侣则呈中等不确定性。这种分布并非偶然，而是精确映射了社交关系转变中的认知复杂性。

衔接词的置信度模式进一步验证了我们的理论框架：模型的犹豫不是简单的词汇理解困难，而是源于处理高语用强度表达时的多维认知任务。在这种情况下，模型既需要理解「关系是什么」（通过名词和动词），又需要把握「关系如何转变」（通过衔接词），这种双重计算负担最终体现为系统性的不确定性。

3.3 实验小结：从现象分析到深层机制

通过语用强度和语义框架的双重分析，我们揭示了模型面对直白表达时的「犹豫」机制。实验结果表明：在处理高语用强度表达时，模型需要同时协调多个语义框架，这种多任务处理显著增加了计算复杂性，导致系统性的不确定性。

这种分析为我们理解模型行为提供了有价值的视角，但要深入探究「为什么」这种现象会发生，我们还需要从更根本的层面思考。具体而言，模型的这种行为可能源于两个深层机制：

统计学习的固有属性：模型基于训练语料中的统计模式运作，在面对高度社交化、高语用强度的表达时，可能需要协调更多相互竞争的概率分布
当下性的缺失：模型缺乏人类处理社交互动时的实时、直觉式语境判断能力，难以快速锁定最合适的解释

这两个深层机制可能共同导致了我们观察到的「犹豫」现象。接下来，我们继续探讨这些机制，进一步理解模型认知的本质限制。

四、模型为何犹豫：从统计机制到当下性缺失

前文分析表明，模型在面对高语用强度的表达时反而表现出更多犹豫——这一表面上的矛盾现象揭示了模型与我们认知之间的根本差异。这种差异，归根结底是两种「理解」方式的鸿沟：一种是基于海量历史的统计推断，另一种是依赖当下情境的直觉感悟，在理解社交语言时，这种差异被无限放大。

4.1 统计机制：确定之中的不确定性

模型本质上是概率分布系统，它通过学习海量文本中的统计规律来预测下一个词。这种机制在处理直白社交表达时产生了一个关键矛盾：越是社交意图明确的表达，在统计分布上可能反而越分散。

对它而言，「想发展爱情关系」这种看似直白的表达，并非直接对应确定的情感状态，而是代表训练语料中大量且高度多元的使用情境：

表白者与接收者之间的多种可能关系（朋友、同事、陌生人）
表达背后的多种潜在意图（真诚告白、试探回应、玩笑测试）
各种可能的后续互动（接受、犹豫、拒绝、转移话题）

尽管「想发展爱情关系」等类似文本可能在训练语料中频繁出现，但大多来自小说、影视剧本、日常对话这些非确定性文本。这导致模型很难清晰区分当下这一句与过往成千上万句的差别，对它而言，一句表白可能有一万种理解。

模型的置信度本质是条件概率分布，即给定前文预测下一词的可能性。当输入「想发展爱情关系」时，模型实际上在海量训练数据中搜索可能的后续场景：这句话可能是真情告白，也可能是开玩笑、暧昧试探或戏谑玩梗。这些后续情境多样且彼此冲突，模型可能在尝试调用更多潜在的情感角色、对话策略以及社交反应，这正是我们在实验中观察到的现象——模型面对直白社交表达时的系统性犹豫。


但这种犹豫并非理解能力不足的表现，恰恰相反，它反映了模型对社交表达多重可能性的全面激活。关键区别在于：人类能够通过当下情境快速过滤不相关可能性，而模型必须同时处理所有统计上可能的解释，导致计算负荷大幅增加。

对模型来说，哲学家维特根斯坦所说的「我语言的边界，就是我世界的边界[8]」，可能并非一种修辞，模型能学到的东西，都来自我们给它的训练语料，这些语料的局限，也就成了模型认识世界的局限。

4.2 当下性：人类直觉的缺失

除了统计学习机制外，模型的犹豫可能源于更深层的原因——缺乏「当下性」。这种当下性是我们能够在特定情境下即时整合信息的能力，帮助我们迅速排除干扰，专注于最符合当前语境的解释。

当听到「想发展爱情关系」这类表达时，我们不仅关注字面含义，还会下意识地整合语调、表情、眼神等多维信息，迅速作出直觉判断。这种直觉类似于我们感知环境的方式：不需构建完整精确的外部模型[9]，而是利用外界本身作为信息存储，通过即时信息检索应对。

从进化角度看，这种直觉能力是我们应对感官模糊性的高效策略，我们在社交互动中本能地运用它。相比之下，即便多模态模型能够「看到」表情或「听到」语调，它们也无法像人类那样下意识地整合这些信息为即时判断[10]，因此在面对复杂的社交意图时总是显得更加犹豫和不确定。

这种差异反映在我们感知和反应上，自然也渗入到语言理解。我们的语言理解并非简单地通过符号与现实对象直接对应，而是通过意义[11]这一中介来实现。

语言理解并非简单的一对一对应，而是涉及符号（语言形式，也称能指）、意义（概念理解）和对象（现实指向，也称所指）三个层面

意义与直觉的结合，让模糊性反而成为沟通优势。就像描述一个人时，「个子中等，穿红色运动鞋」比「身高1米75，体重68公斤」更容易让人找到。精确数字虽然客观，却难以与感知经验关联；模糊描述却能迅速激活我们的意义网络。帮助我们完成从符号（语言）到对象（现实世界）的快速且准确的映射。

相比之下，模型缺少「意义」的中介环节，仅通过统计关联将语言符号与现实对象连接。这种根本差异导致模型在处理社交意图时，缺乏像人类那样灵活应对语言模糊性和情境的能力。

以我们的实验为例，在个人知识中，模型将我家理解为普通地点，夜晚作为普通时间，完全错过了暧昧邀约的潜台词。这是因为它跳过了「意义」层，直接连接符号与对象。而面对共同知识中的「想发展爱情关系」这样的直白表达，模型则通过穷举可能含义来应对，导致显著的犹豫不决。

短时间内，模型无法具备人类的直觉，根本原因在于其设计基础。计算机与数学语言被设计来消除不确定性，而我们的感官、语言（或世界）本身充满模糊性。正是这种模糊性，使我们进化出了直觉能力：在不完美知识框架下做出迅速且正确的判断。这正是模型所缺乏的，也是我们暂时无法轻易传授给它的。

五、结语

本文揭示了一个反直觉的现象：模型在处理直白表达时反而表现出更多不确定性。这种现象的根源在于模型统计学习机制的内在局限，以及缺乏人类独有的「当下性」直觉。

这些发现还引导我们思考语言的边界究竟在哪里。通过观察模型处理暧昧表达的方式，我们意识到：有些话，并非不应被说出，而是无法被说出。那些最深刻的理解，往往存在于语言之外，只能通过感受与直觉传达，这或许就是语言的边界所在。

回到《窈窕淑男》中的朱莉，当她向「闺蜜」倾诉对赤裸表白的渴望时，她究竟想要什么？是一番云雨？还是一种被理解、被接纳的感受？抑或只是一个能让她放下防备，坦诚面对自我的空间，无论她说什么，都能被温柔以待。而这个空间，恰恰是那个虚构身份的「多萝西」——陪伴朱莉度过欢笑与泪水的「闺蜜」，却无比真诚的迈克尔给予她的。

迈克尔（男扮女装的多萝西）与朱莉──《窈窕淑男》

当故事最后迈克尔说「我怀念多萝西」时，朱莉无需再解读这句话的含义了，因为她也同样怀念。

这一切，又何须语言学说明呢？
参考资料
[1] 

Kittler, M. G., Rygl, D., and Mackinnon, A. "Beyond Culture or Beyond Control? Reviewing the Use of High-/Low-Context Concept." International Journal of Cross Cultural Management, vol. 11, no. 1, 2011, pp. 63-82.【该研究质疑以往将文化表达方式以地区划分的做法（如东亚高语境论认为东亚人更偏爱含蓄表达），认为这一分类依据不足，相关研究结论往往混杂且矛盾，表明文化差异的界定远比传统认知更为复杂。】

[2] 

Brown, Penelope, and Stephen C. Levinson. "Universals in Language Usage: Politeness Phenomena." Questions and Politeness: Strategies in Social Interaction, edited by Esther N. Goody, Cambridge University Press, 1978, pp. 56-311.

[3] 

Pinker, Steven. The Stuff of Thought: Language as a Window into Human Nature. 1st ed., Viking, 2007.

[4] 

深度学习奠基人辛顿（Geoffrey E. Hinton）在诺奖采访中表示，神经网络（大语言模型）的语言处理方式更接近人类的认知模式，他批评乔姆斯基等语言学家提出的「人类与大模型在语言理解上截然不同」这一观点。

[5] 

该模型知识逻辑与意图识别能力较强，实验环境为 NVIDIA A800 显卡。实验共进行 10 次，最终选取平均样本版本，并采用采样机制，避免固定结果，确保实验更具参考价值。

[6] 

言语行为与语用强度：奥斯汀（Austin 1962）与塞尔勒（Searle 1969）提出言语行为理论，区分「句子义」（sentence meaning）与「语句义」（utterance meaning），强调语言不仅传递信息，还具有行动功能。其中，施为力（illocutionary force）决定表达的影响力。我们在此基础上延展出「语用强度」概念，探讨表达的直接性如何塑造交际关系。实验显示，语用强度越高，大模型的置信度反而降低，表现出更多不确定性。这表明语用强度不仅影响人际交流，也可作为分析模型语言理解的视角。

[7] 

语义框架与不确定性：框架语义学（Fillmore 1980s）认为词义依赖特定认知框架，尽管无法证实模型具有人类般的认知框架，但其注意力机制却呈现出类似特性。——词义的确定性受上下文语境制约。分析模型在关键节点上的「犹豫」现象，揭示了高语用强度表达对模型认知负担的影响，且这种负担集中于特定词汇。这表明，直白的表达方式反而增加了模型在社交意图推断上的不确定性。

[8] 

维特根斯坦，路德维希. 逻辑哲学论. 商务印书馆，1996年。

[9] 

Clark, Andy. Natural-Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence. Oxford University Press, 2004.

[10] 

Yang, Jihan, et al. "Thinking in Space: How Multimodal Large Language Models See, Remember, and Reason." arXiv, 18 Dec. 2024, arxiv.org/abs/2412.14171.【李飞飞团队探讨多模态大模型（MLLMs）在视觉空间智能中的表现，尤其是视频数据的感知与推理。研究发现，这些模型虽经大规模数据训练，但空间推理能力远不及人类，缺乏动态整合信息的本能。】

[11] 

Ogden, C. K., and I. A. Richards. The Meaning of Meaning: A Study of the Influence of Language upon Thought and of the Science of Symbolism. Harcourt, Brace & World, 1923.

​

Scan to Follow
