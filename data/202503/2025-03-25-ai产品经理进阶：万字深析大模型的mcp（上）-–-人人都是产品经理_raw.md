Title: AI产品经理进阶：万字深析大模型的MCP（上） – 人人都是产品经理,

URL Source: https://www.woshipm.com/ai/6190336.html

Published Time: 2025-03-11T09:03:22

Markdown Content:
> 随着人工智能技术的飞速发展，如何让AI系统更高效地与外部数据源和工具进行交互，成为了一个亟待解决的问题。本文将深入探讨一种名为MCP（Model Context Protocol）的新兴技术，供大家学习。

![Image 1](https://image.woshipm.com/2023/04/13/f35c645e-d9dd-11ed-8440-00163e0b5ff3.jpg)

做Agent产品抵达深水区时，你总会遇到**MCP(Model Context Protocol)**这个怪兽，今天我们一起来打掉他。

简单理解：

MCP的核心价值就在于**统一标准、消除隔阂**。

它把AI和外界连接的方式从“烟囱林立”变成了“道路纵横”，让信息和指令能够自由流通。

让我们先通过几个熟悉的场景建立一个感性的认知：

通俗类比解释MCP
---------

### “万能插座”

可以把MCP想象成给所有AI程序制定了统一的“插座标准”。过去，不同的AI（插头）要连接不同的数据源（电源），往往需要各种各样的转换适配器；而MCP就好比全国统一了插座形状，让任何AI插头都能直接插入任何数据源插座使用。这个统一接口大大方便了AI取用数据，就像电器有了标准电源后随处都能通电一样。

### “通用翻译官”

MCP又像是AI与外界系统之间的**通用语言翻译官**。每种数据源本来有自己的“语言”（接口/API），每个AI模型也有自己的“理解方式”。MCP制定了一种大家都懂的语言：数据源把信息用MCP语言表达出来，AI也用MCP语言提出请求。这样双方即使“母语”不同，也能在MCP翻译下无障碍沟通。比如AI不会直接说“给我文件X”，而是通过MCP提出标准化请求，文件系统也通过MCP以标准格式回应。所有交流都有共同的语法和词汇，避免了误解和交流障碍。

### “AI的USB-C接口”

正如前文所述，官方将MCP比作AI领域的USB-C接口。类比来看，不同的AI助手就像不同的电子设备，以前每个设备需要不同的数据线连不同的外设（比如老式手机数据线各不相同），而MCP提供了一个统一的细窄接口，让AI能够**即插即用各种外设**。例如，通过MCP，一个AI助手今天可以连U盘（数据库），明天插打印机（邮件系统），后天接显示器（报告生成）——接口都一样，只是功能不同。就像USB-C让我们少了无数转换头和线缆，MCP也让AI集成少了无数专有API和脚本。对于终端用户来说，这意味着AI助手将变得**更加多才多艺**且使用方便，因为背后复杂的连接都被这个看不见的“USB-C”标准屏蔽掉了。

MCP的基本概念和定义
-----------

**Model Context Protocol(MCP)**是由Anthropic在2024年底提出并开源的一种开放标准协议，用于连接大型语言模型（LLM）等AI系统与各种外部数据源和工具。简单来说，MCP为AI模型提供了一个统一接口，让模型能够方便、安全地访问所需的数据和功能，而不再被“困”在封闭的知识范围内。官方将MCP比喻为AI应用的“USB-C接口”——正如USB-C为电子设备提供了标准化的连接方式，MCP也提供了标准化方式将AI模型连接到不同的数据源和工具。换句话说，以前每新增一种数据源都需要定制开发对接代码，而现在有了MCP这个“通用插头”，AI与任何数据源对接都可以按照统一规则进行。

MCP本质上定义了一套**通用语言和接口**，用于让AI模型与外部系统交换“上下文”信息（即模型需要的额外信息或操作指令）。例如，通过MCP，像Claude这样的对话模型可以直接查询数据库、调用API、读取文件，获取实时的信息用于回答问题。MCP的目标是提高AI助手响应的相关性和准确性，让模型的知识和能力不再局限于训练语料，而是能动态扩展到用户自己的数据和现有工具中。正因如此，可以把MCP视作一个**AI领域的通用翻译器**：Anthropic公司人士形象地描述，他们的愿景是“构建一个AI可以连接任意数据源的世界”，而MCP则充当了其中的“通用翻译器”。

起源与发展背景
-------

MCP诞生于AI应用迅猛发展的背景下。随着GPT-4、Claude等大模型在推理能力和回答质量上取得突破，如何让**AI与企业自身的数据和系统对接**成为新的挑战。在MCP出现之前，不同AI助手接入数据库、文件系统、第三方应用等通常都需要编写定制的集成代码。例如，开发者可能使用LangChain框架编写Python脚本，让某个LLM能查询自家数据库；而如果换成另一个LLM模型或另一个数据源，又要重新适配新的代码。这种**“M×N集成难题”**日益突出：M种不同的大模型与N种不同的数据/工具要两两集成，需要开发和维护大量碎片化的连接器。这不仅开发成本高、容易出错，而且难以规模化地扩展AI系统的能力。

为了解决上述痛点，Anthropic于**2024年11月25日**正式发布并开源了MCP规范和SDK，尝试将其打造为AI数据集成的行业标准。Anthropic在发布MCP时强调，它是一个“**通用开放标准**”，旨在取代过去碎片化的集成方式，为AI系统访问数据提供一种更简单可靠的方案。MCP通过标准化LLM与外部资源交互的接口，试图像USB或HTTP那样建立统一的“协议层”，从根本上减少重复劳动和集成复杂度。Anthropic的工程师在社区讨论中也指出，希望MCP能够真正解决“每个模型对接每个工具”需要单独实现的问题，从而让业界**一次集成、处处运行**。

作为一项开放项目，MCP从一开始就采取开源协作的模式。Anthropic提供了详细的协议规范、参考实现以及多种语言的SDK（如Python、TypeScript、Java等）供开发者使用。同时，他们建立了开放的连接器仓库，鼓励社区贡献各种数据源的MCP服务器实现。在发布之初，Anthropic已公布了Google Drive、Slack、GitHub、Git、Postgres数据库、Puppeteer等常用系统的**预构建MCP连接器**，方便开发者快速将Claude这类AI助手接入这些系统。一些早期采用者（如Block公司和Apollo等）已将MCP集成到自己的平台中，另外如Zed、Replit、Codeium、Sourcegraph等开发工具厂商也在与MCP合作，利用它让AI代理更好地检索编码任务相关的上下文信息。不过需要注意的是，**当前MCP主要在Claude系列模型和部分支持MCP的客户端中使用**。虽然MCP面向所有AI厂商开放，但要真正成为通用标准，还取决于更多模型提供商和应用开发者的支持。这也是MCP未来面临的一个重要挑战：推动行业广泛认可其价值，将“开放标准”转化为事实上的标准。

MCP的核心原理和技术架构
-------------

### **架构概览**

MCP采用**客户端-服务器**的分布式架构。具体而言，包括以下组件：

*   **MCP Host（主机应用）**：运行AI模型或代理的宿主程序，如Claude桌面版、某IDE中的AI助手等。主机应用通过内置的MCP客户端与外部建立连接，是连接的发起方。
*   **MCP Client（客户端）**：嵌入在主机应用中的协议客户端组件。每个MCP客户端与一个特定的MCP服务器保持一对一的连接，用于向服务器发送请求或接收响应。一个主机应用中可以运行多个MCP客户端，从而同时连接多个不同的服务器。
*   **MCP Server（服务器）**：独立运行的轻量程序，封装了某一数据源或服务的具体能力，通过标准化的MCP接口对外提供。每个MCP服务器相当于一个“适配器”，将底层的数据源/工具（本地文件、数据库、第三方API等）的功能以统一格式暴露出来，供客户端调用。
*   **本地数据源**：部署在用户本地环境中的数据资源，例如本机文件系统、数据库、应用服务等。MCP服务器可以受控地访问这些资源，并将内容提供给AI模型使用。例如，一个本地MCP服务器可以读取电脑文件或查询本地SQLite数据库，然后将结果发给模型作为参考。
*   **远程服务**：通过网络提供的外部系统或在线服务（通常通过HTTP API访问）。MCP服务器也可以连接到这些远程服务获取数据。比如，可以有一个MCP服务器连接Slack的Web API，代表AI助手执行发送消息、读取频道记录等操作。

这种架构下，**AI主机通过MCP客户端同时连接多个MCP服务器**，每个服务器各司其职，提供对一种数据源或应用的标准化接入。这样设计有几个好处：一是**模块化**，增加或移除某个数据源只需启用或停用对应的服务器，不影响AI主体或其他部分；二是**解耦**，AI模型与具体数据源实现隔离开，通过协议交互，不直接依赖数据源的内部细节；三是**双向通信**，不仅AI可以请求数据源，某些情况下数据源也能要求AI执行操作或生成内容，从而支持更复杂的交互流程。

### 通信方式

MCP定义了一套基于JSON-RPC 2.0的**消息通信协议**。底层传输层可以灵活选择，目前提供了两种标准传输方式：一是**标准输入输出（STDIO）**，适用于本地集成（通过进程管道通信）；二是**服务器发送事件（SSE）配合HTTP POST，用于需要通过HTTP网络连接的场景。开发者也可以实现自定义传输方式，只要符合MCP传输接口要求即可。采用JSON-RPC意味着消息以JSON格式封装，包括请求（request）、响应（response）和通知（notification）三种类型，每条消息带有方法名称和参数等。这种设计使得MCP通信类似远程过程调用（RPC）**，方便表达“调用某工具”“读取某资源”等操作，而且因为使用JSON，调试和扩展都比较方便（相较于gRPC这类二进制协议，更易于人阅读和日志记录）。需要注意的是，MCP协议在消息级别管理请求-响应的关联、错误处理等，从而保证多路并发调用时也能正确对应。

**关键机制 – “Primitives”（原语）概念：** MCP将AI与外部系统交互的内容抽象为几类**原语**，以此规范客户端和服务器各自能提供的功能。具体来说，**MCP服务器**可以提供三种原语：

*   **Prompts（提示）**：预先编写的提示词或模板，相当于一段指导性文字片段，可以插入到模型的输入中去影响其行为。例如服务器可以提供一个“代码审查提示模板”，供模型在阅读代码时使用。
*   **Resources（资源）**：结构化的数据或文档内容，可供客户端读取并提供给模型作为上下文。例如从数据库查询到的一条记录、用户的笔记文档内容等，都是资源类型。资源类似于“只读文件”，模型可以请求某个资源，服务器会返回相应的数据内容。
*   **Tools（工具）**：可以被模型调用的可执行操作或函数。这是MCP最强大也最具互动性的部分，模型可以要求服务器执行某个工具函数来获取信息或改变外部状态，比如调用“发送邮件”工具发送一封邮件，调用“查询天气”工具获取天气数据等。由于工具调用可能带来副作用和安全风险，**MCP规定模型调用工具必须经由用户批准**后才执行。换言之，工具就像模型可用的“按键”，但每次按键需要真人确认，避免模型滥用外部操作权限。

对应地，**MCP客户端**提供两种原语能力用于辅助服务器完成复杂任务：

*   **Roots（根）**：这是一种由客户端提供的文件系统入口或句柄。服务器可以通过Root来访问客户端这侧的本地文件或目录内容。例如客户端可以授权服务器读取某个文件夹（作为Root），那么服务器就能代表模型浏览那个文件夹下的文件内容（通常仍以资源形式提供给模型）。Roots机制确保服务器只能访问经授权的本地数据范围，增强安全性。
*   **Sampling（采样）**：这一机制允许服务器向客户端发起请求，要求客户端这侧的LLM模型生成一段文本（即一次补全/推理）。简单说，服务器也可以“反过来”调用模型，让模型基于一些额外提示执行推理。Sampling可以用于构建多轮交互的智能Agent：服务器在执行某工具过程中，发现需要模型进一步推理决定下一步时，就可以用Sampling请求模型产出结果，再继续后续操作。不过Anthropic也强调应谨慎使用这一机制，**始终保持人类在环**监督，以避免AI代理失控循环调用模型

通过上述原语分类，MCP清晰地定义了模型与外部交互的**意图类型**。例如，让模型获取一段参考资料应该作为Resource提供，而不是混同于调用Tool；又如要求模型执行某操作就用Tool明确表示。这样的设计使AI系统的上下文管理更结构化：模型知道某段信息是只读资料还是可执行操作，用户也能对不同类型请求进行针对性地审批或监控。这比起简单地给模型一个隐式“工具插件”要透明得多。Anthropic的开发者指出，他们最初也考虑过是否把所有交互都当作“工具调用”统一处理，但最终认为**Prompt和Resource这两类原语有其独特意义**，能表达不同用途的功能，因此保留了多元的原语概念。

### 工作流程

在实际运行中，当用户向AI助手提出一个需要外部信息或操作的问题时，MCP让这一过程变得自动又安全：

1.  **能力发现**：首先，主机应用启动时会连接配置好的若干MCP服务器。每个服务器会向客户端声明自己提供的Prompts、Resources、Tools列表（通常通过JSON模式定义）及其调用方法等信息。这样模型在对话过程中，就知道有哪些“工具”可用、有哪些资源可以读取。例如，一个Slack服务器可能声明工具`list_channels`（列出频道）、`post_message`（发消息）等。
2.  **模型决策调用**：当用户提问需要外部支持时，模型根据提示和上下文，可能决定调用某个工具或请求某个资源。由于大模型可以生成结构化输出，Claude等模型能够按照MCP约定格式提出调用请求（这通常由MCP客户端在模型生成内容中解析出调用意图并执行）。例如用户要求“请把这篇文章发到Slack频道X”，模型会触发调用Slack服务器的`post_message`工具，并附上文章内容和频道参数。
3.  **用户审批执行**：MCP客户端接收到模型的调用请求后，会提示人类用户（例如在Claude桌面版界面中）确认是否允许执行该操作。如果被拒绝，则向模型返回错误信息；如果允许，则将请求发送给对应的MCP服务器。对于纯读取类的Resource请求，有时可按策略自动批准。
4.  **服务器处理并响应**：MCP服务器收到请求后，执行实际逻辑。例如Slack服务器调用Slack API发送消息，文件服务器读取文件内容等。执行完成后，服务器将结果封装为响应（或资源内容）通过MCP协议发回客户端。如果在执行过程中需要模型帮助（例如对长文本进行总结），服务器还可以通过Sampling请求模型生成，待模型给出结果后再继续。
5.  **模型利用结果继续对话**：客户端收到服务器响应后，将结果提供给模型作为新的上下文。模型据此产生最终回答或下一步动作。例如Slack消息发送完成后，Claude可以告诉用户“文章已成功发布到频道X”。对于Resource类数据，模型会将其内容纳入回答的依据，从而给出更准确的答复。

通过以上机制，MCP在**AI模型与外部环境之间搭建了一个标准化、可控的“桥梁”**。它既允许模型扩展能力，与真实世界的数据和应用交互，又通过明确的接口契约和用户权限把控，避免了安全风险。正如一位观察者所言，MCP的出现使得AI助手有望变得**更自主智能**，因为它们可以维护跨不同工具和数据源的上下文，帮助用户执行更复杂的任务。

在技术实现细节上，MCP的设计还借鉴了许多已有成熟理念。例如，其JSON-RPC消息结构与**语言服务器协议（LSP）有相似之处（LSP也是使用JSON-RPC让编辑器与编译器服务通信）；又如将工具调用与用户审批结合，这和OpenAI API中的“函数调用”+人类验证思路类似，只是MCP做得更通用、更系统化。此外，MCP还内置了安全机制（如严格的权限范围、错误码等）来确保鲁棒性。例如对于每个连接，客户端和服务器都会进行能力协商和版本匹配**，保证双方理解的协议一致；传输层也提供了错误处理约定，防止死锁或资源泄露。

整体来看，MCP核心架构通过**标准协议+客户端/服务器分层**，将AI与外部世界有效衔接起来。这种模式让开发者“**只需针对MCP编程一次，便可让各种模型与各种数据源对接**”，显著降低了AI应用集成的门槛。

本文由 @AI贾维斯 原创发布于人人都是产品经理。未经作者许可，禁止转载

题图来自Unsplash，基于CC0协议

该文观点仅代表作者本人，人人都是产品经理平台仅提供信息存储空间服务
