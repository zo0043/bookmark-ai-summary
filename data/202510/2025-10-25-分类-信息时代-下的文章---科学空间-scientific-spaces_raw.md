Title: 分类 信息时代 下的文章 - 科学空间|Scientific Spaces

URL Source: https://kexue.fm/category/Big-Data

Markdown Content:
分类 信息时代 下的文章 - 科学空间|Scientific Spaces

===============

[![Image 4: MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)](https://kexue.fm/category/Big-Data)

SEARCH
------

MENU
----

*   [打赏](https://kexue.fm/reward.html)
*   [公式](https://kexue.fm/latex.html)
*   [天象](https://kexue.fm/ac.html)
*   [链接](https://kexue.fm/links.html)
*   [时光](https://kexue.fm/me.html)
*   [博览](https://kexue.fm/science.html)
*   [归档](https://kexue.fm/content.html)

CATEGORIES
----------

*   [千奇百怪](https://kexue.fm/category/Everything)
*   [天文探索](https://kexue.fm/category/Astronomy)
*   [数学研究](https://kexue.fm/category/Mathematics)
*   [物理化学](https://kexue.fm/category/Phy-chem)
*   [信息时代](https://kexue.fm/category/Big-Data)
*   [生物自然](https://kexue.fm/category/Biology)
*   [图片摄影](https://kexue.fm/category/Photograph)
*   [问题百科](https://kexue.fm/category/Questions)
*   [生活/情感](https://kexue.fm/category/Life-Feeling)
*   [资源共享](https://kexue.fm/category/Resources)

NEWPOSTS
--------

*   [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
*   [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
*   [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
*   [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
*   [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
*   [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
*   [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
*   [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
*   [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
*   [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)

COMMENTS
--------

*   [大力: 你好，苏老师请问（14）式子中β2tβ¯2tEε∼N(0,I)...](https://kexue.fm/archives/9119/comment-page-13#comment-28682)
*   [苏剑林: 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上...](https://kexue.fm/archives/11340/comment-page-1#comment-28680)
*   [苏剑林: 可以这样理解：t t 时刻的 x t x t...](https://kexue.fm/archives/9257/comment-page-4#comment-28679)
*   [苏剑林: 没有太多技巧了，就是直接代入然后根据$\bar{\alpha}...](https://kexue.fm/archives/9181/comment-page-5#comment-28678)
*   [苏剑林: 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那...](https://kexue.fm/archives/10862/comment-page-1#comment-28677)
*   [Zhan-Wang Mao: 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和 t t 有关...](https://kexue.fm/archives/9257/comment-page-4#comment-28676)
*   [yzlnew: 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和...](https://kexue.fm/archives/11340/comment-page-1#comment-28675)
*   [Henry: 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？](https://kexue.fm/archives/9181/comment-page-5#comment-28673)
*   [szsheep: 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度...](https://kexue.fm/archives/9119/comment-page-13#comment-28672)
*   [pang: 对于目前的MLA算法softmax(X×WQ×WukT×CjT...](https://kexue.fm/archives/10862/comment-page-1#comment-28671)

USERLOGIN
---------

*   [登录](https://kexue.fm/admin/login.php)

[科学空间|Scientific Spaces](https://kexue.fm/)
===========================================

*   [登录](https://kexue.fm/admin/login.php)
*   [打赏](https://kexue.fm/reward.html)
*   [公式](https://kexue.fm/latex.html)
*   [天象](https://kexue.fm/ac.html)
*   [链接](https://kexue.fm/links.html)
*   [时光](https://kexue.fm/me.html)
*   [博览](https://kexue.fm/science.html)
*   [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

*   [![Image 5](https://kexue.fm/usr/themes/geekg/images/rss.png) 欢迎订阅](https://kexue.fm/feed)
*   [![Image 6](https://kexue.fm/usr/themes/geekg/images/mail.png) 个性邮箱](https://kexue.fm/archives/119)
*   [![Image 7](https://kexue.fm/usr/themes/geekg/images/Saturn.png) 天象信息](https://kexue.fm/ac.html)
*   [![Image 8](https://kexue.fm/usr/themes/geekg/images/iss.png) 观测ISS](https://kexue.fm/archives/41)
*   [![Image 9](https://kexue.fm/usr/themes/geekg/images/pi.png) LaTeX](https://kexue.fm/latex.html)
*   [![Image 10](https://kexue.fm/usr/themes/geekg/images/mlogo.png) 关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

*   [**千奇百怪**Everything](https://kexue.fm/category/Everything)
*   [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
*   [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
*   [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
*   [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
*   [**生物自然**Biology](https://kexue.fm/category/Biology)
*   [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
*   [**问题百科**Questions](https://kexue.fm/category/Questions)
*   [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
*   [**资源共享**Resources](https://kexue.fm/category/Resources)

*   [**千奇百怪**](https://kexue.fm/category/Everything)
*   [**天文探索**](https://kexue.fm/category/Astronomy)
*   [**数学研究**](https://kexue.fm/category/Mathematics)
*   [**物理化学**](https://kexue.fm/category/Phy-chem)
*   [**信息时代**](https://kexue.fm/category/Big-Data)
*   [**生物自然**](https://kexue.fm/category/Biology)
*   [**图片摄影**](https://kexue.fm/category/Photograph)
*   [**问题百科**](https://kexue.fm/category/Questions)
*   [**生活/情感**](https://kexue.fm/category/Life-Feeling)
*   [**资源共享**](https://kexue.fm/category/Resources)

21 Oct

[MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
----------------------------------------------------

By 苏剑林 | 2025-10-21 | 5058位读者 | [引用](https://kexue.fm/archives/11340#how_to_cite)

不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“**M**oment**U**m Orthogonalized by Newton-Schulz”，后者是“**M**aximal **U**pdate Parametrization”，可它们俩之间确实有着非常深刻的联系。也就是说，Muon和MuP有着截然不同的出发点，但最终都走向了相同的方向，甚至无意间取了相似的名字，似乎真应了那句“冥冥中自有安排”。

言归正传。总之，笔者在各种机缘巧合之下，刚好同时学习到了Muon和MuP，这大大加深了笔者对模型优化的理解，同时也让笔者开始思考关于模型优化更本质的原理。经过一段时间的试错，算是有些粗浅的收获，在此跟大家分享一下。

写在前面
----

按照提出时间的先后顺序，是先有MuP再有Muon，但笔者的学习顺序正好反过来，先学习了Muon然后再学习MuP，事后来看，这也不失为一个不错的学习顺序。

[点击阅读全文...](https://kexue.fm/archives/11340 "MuP之上：1. 好模型的三个特征")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [尺度定律](https://kexue.fm/tag/%E5%B0%BA%E5%BA%A6%E5%AE%9A%E5%BE%8B/), [MuP](https://kexue.fm/tag/MuP/)[阅读全文](https://kexue.fm/archives/11340)[2 评论](https://kexue.fm/archives/11340#comments)

8 Oct

[DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
------------------------------------------------------

By 苏剑林 | 2025-10-08 | 14636位读者 | [引用](https://kexue.fm/archives/11328#how_to_cite)

对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文[《Neural Discrete Representation Learning》](https://papers.cool/arxiv/1711.00937)，笔者在2019年的博客[《VQ-VAE的简明介绍：量子化自编码器》](https://kexue.fm/archives/6760)也介绍过它。

然而，这么多年过去了，我们可以发现VQ的训练技术几乎没有变化，都是STE（Straight-Through Estimator）加额外的Aux Loss。STE倒是没啥问题，它可以说是给离散化运算设计梯度的标准方式了，但Aux Loss的存在总让人有种不够端到端的感觉，同时还引入了额外的超参要调。

幸运的是，这个局面可能要结束了，上周的论文[《DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick》](https://papers.cool/arxiv/2509.26469)提出了一个新的STE技巧，它最大亮点是不需要Aux Loss，这让它显得特别简洁漂亮！

[点击阅读全文...](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [编码](https://kexue.fm/tag/%E7%BC%96%E7%A0%81/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [离散化](https://kexue.fm/tag/%E7%A6%BB%E6%95%A3%E5%8C%96/)[阅读全文](https://kexue.fm/archives/11328)[2 评论](https://kexue.fm/archives/11328#comments)

5 Oct

[为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
--------------------------------------------------------

By 苏剑林 | 2025-10-05 | 24009位读者 | [引用](https://kexue.fm/archives/11320#how_to_cite)

如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033)）模型都给 Q,K,V Q,K,V 加上了Short Conv，比如下图所示的[DeltaNet](https://papers.cool/arxiv/2406.06484)：

[![Image 11: DeltaNet中的Short Conv](https://kexue.fm/usr/uploads/2025/10/175536171.png)](https://kexue.fm/usr/uploads/2025/10/175536171.png "点击查看原图")

DeltaNet中的Short Conv

为什么要加这个Short Conv呢？直观理解可能是增加模型深度、增强模型的Token-Mixing能力等，说白了就是补偿线性化导致的表达能力下降。这个说法当然是大差不差，但它属于“万能模版”式的回答，我们更想对它的生效机制有更准确的认知。

接下来，笔者将给出自己的一个理解（更准确说应该是猜测）。

[点击阅读全文...](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[线性](https://kexue.fm/tag/%E7%BA%BF%E6%80%A7/), [RNN](https://kexue.fm/tag/RNN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[阅读全文](https://kexue.fm/archives/11320)[3 评论](https://kexue.fm/archives/11320#comments)

25 Aug

[Cool Papers更新：简单适配Zotero Connector](https://kexue.fm/archives/11250)
---------------------------------------------------------------------

By 苏剑林 | 2025-08-25 | 24088位读者 | [引用](https://kexue.fm/archives/11250#how_to_cite)

很早之前就有读者提出希望可以给[Cool Papers](https://papers.cool/)增加导入Zotero的功能，但由于笔者没用Zotero，加上又比较懒，所以一直没提上日程。这个周末刚好有点时间，研究了一下，做了个简单的适配。

单篇导入
----

首先，我们需要安装[Zotero](https://www.zotero.org/)（这是废话），然后需要给所用浏览器安装[Zotero Connector](https://www.zotero.org/download/connectors)插件。安装完成后，我们访问Cool Papers的单篇论文页面，如 [https://papers.cool/arxiv/2104.09864](https://papers.cool/arxiv/2104.09864) 或 [https://papers.cool/venue/2024.naacl-long.431@ACL](https://papers.cool/venue/2024.naacl-long.431@ACL) ，然后点击Zotero Connector的图标，就会自动把论文导入了，包括PDF文件。

[![Image 12: 单篇论文导入到Zotero](https://kexue.fm/usr/uploads/2025/08/2053963362.png)](https://kexue.fm/usr/uploads/2025/08/2053963362.png "点击查看原图")

单篇论文导入到Zotero

[点击阅读全文...](https://kexue.fm/archives/11250 "Cool Papers更新：简单适配Zotero Connector")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/), [论文](https://kexue.fm/tag/%E8%AE%BA%E6%96%87/), [酷论文](https://kexue.fm/tag/%E9%85%B7%E8%AE%BA%E6%96%87/)[阅读全文](https://kexue.fm/archives/11250)[4 评论](https://kexue.fm/archives/11250#comments)

2 Aug

[基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)
-------------------------------------------------------

By 苏剑林 | 2025-08-02 | 18003位读者 | [引用](https://kexue.fm/archives/11206#how_to_cite)

前段时间搞了个很迷你的开发板树莓派Zero2W（下面简称“**Pi**”），还搭配了个USB Key转接板，这几天折腾了一下，用于实现一个随身的旁路由。本文记录一下关键技术点，供有同样需求的读者参考。

[![Image 13: 树莓派Zero2W](https://kexue.fm/usr/uploads/2025/08/1443796894.jpeg)](https://kexue.fm/usr/uploads/2025/08/1443796894.jpeg "点击查看原图")

树莓派Zero2W

[点击阅读全文...](https://kexue.fm/archives/11206 "基于树莓派Zero2W搭建一个随身旁路由")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[linux](https://kexue.fm/tag/linux/), [网络](https://kexue.fm/tag/%E7%BD%91%E7%BB%9C/), [路由器](https://kexue.fm/tag/%E8%B7%AF%E7%94%B1%E5%99%A8/), [智能家居](https://kexue.fm/tag/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/)[阅读全文](https://kexue.fm/archives/11206)[2 评论](https://kexue.fm/archives/11206#comments)

12 Jul

[QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
---------------------------------------------------------------

By 苏剑林 | 2025-07-12 | 71503位读者 | [引用](https://kexue.fm/archives/11126#how_to_cite)

四个月前，我们发布了[Moonlight](https://kexue.fm/archives/10739)，在16B的MoE模型上验证了[Muon](https://kexue.fm/archives/10592)优化器的有效性。在Moonlight中，我们确认了给Muon添加Weight Decay的必要性，同时提出了通过Update RMS对齐来迁移Adam超参的技巧，这使得Muon可以快速应用于LLM的训练。然而，当我们尝试将Muon进一步拓展到千亿参数以上的模型时，遇到了新的“拦路虎”——MaxLogit爆炸。

为了解决这个问题，我们提出了一种简单但极其有效的新方法，我们称之为“QK-Clip”。该方法从一个非常本质的角度去看待和解决MaxLogit现象，并且无损模型效果，这成为我们最新发布的万亿参数模型“[Kimi K2](https://moonshotai.github.io/Kimi-K2/)”的关键训练技术之一。

问题描述
----

我们先来简单介绍一下MaxLogit爆炸现象。回顾Attention的定义

O=s o f t m a x(Q K⊤)V(1)(1)O=s o f t m a x(Q K⊤)V

[点击阅读全文...](https://kexue.fm/archives/11126 "QK-Clip：让Muon在Scaleup之路上更进一步")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [attention](https://kexue.fm/tag/attention/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [muon](https://kexue.fm/tag/muon/)[阅读全文](https://kexue.fm/archives/11126)[60 评论](https://kexue.fm/archives/11126#comments)

10 Jul

[Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
-----------------------------------------------------------------

By 苏剑林 | 2025-07-10 | 58451位读者 | [引用](https://kexue.fm/archives/11111#how_to_cite)

在文章[《Transformer升级之路：20、MLA好在哪里?（上）》](https://kexue.fm/archives/10907)中，我们对[MLA](https://kexue.fm/archives/10091)相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享”，实验的初步结果是这三个变化很可能都是MLA效果优异的原因。

本文我们将从一个更加偏理论的角度出发，来理解MLA的成功之处。

部分旋转
----

首先，我们把最终的断言放在前面：

> 在相同训练成本和推理成本下，MLA可能是效果最好的Full Attention变体。

[点击阅读全文...](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[阅读全文](https://kexue.fm/archives/11111)[53 评论](https://kexue.fm/archives/11111#comments)

20 Jun

[线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
----------------------------------------------------

By 苏剑林 | 2025-06-20 | 51685位读者 | [引用](https://kexue.fm/archives/11033#how_to_cite)

在中文圈，本站应该算是比较早关注线性Attention的了，在2020年写首篇相关博客[《线性Attention的探索：Attention必须有个Softmax吗？》](https://kexue.fm/archives/7546)时，大家主要讨论的还是BERT相关的Softmax Attention。事后来看，在BERT时代考虑线性Attention并不是太明智，因为当时训练长度比较短，且模型主要还是Encoder，用线性Attention来做基本没有优势。对此，笔者也曾撰文[《线性Transformer应该不是你要等的那个模型》](https://kexue.fm/archives/8610)表达这一观点。

直到ChatGPT的出世，倒逼大家都去做Decoder-only的生成式模型，这跟线性Attention的RNN形式高度契合。同时，追求更长的训练长度也使得Softmax Attention的二次复杂度瓶颈愈发明显。在这样的新背景下，线性Attention越来越体现出竞争力，甚至出现了“反哺”Softmax Attention的迹象。

[点击阅读全文...](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")

分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[线性](https://kexue.fm/tag/%E7%BA%BF%E6%80%A7/), [RNN](https://kexue.fm/tag/RNN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[阅读全文](https://kexue.fm/archives/11033)[43 评论](https://kexue.fm/archives/11033#comments)

    1.   [1](https://kexue.fm/category/Big-Data/1/)
    2.   [2](https://kexue.fm/category/Big-Data/2/)
    3.   [3](https://kexue.fm/category/Big-Data/3/)
    4.   [4](https://kexue.fm/category/Big-Data/4/)
    5.   ...
    6.   [52](https://kexue.fm/category/Big-Data/52/)
    7.   [»](https://kexue.fm/category/Big-Data/2/)

### 关于站长

![Image 14: 科学空间logo](https://kexue.fm/usr/themes/geekg/images/avatar.png)

**苏剑林|BoJone**，科学空间博主，【数学、天文、理论物理、写作、阅读、计算机、中国象棋、厨房】爱好者（但不专业）......目前32岁，还在单调递增。希望能一直在此分享科学之美～

**你也许会关心：***   [科学空间|Scientific Spaces 介绍](https://kexue.fm/me.html)
*   科学空间QQ交流群：67729435
*   科学空间微信交流群：[spaces_ac_cn](https://kexue.fm/archives/4096/)
*   常见问题集：[《科学空间FAQ》](https://kexue.fm/archives/6508/)

### 智能搜索

支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/)[语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)[模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/)[网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/)[概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/)[梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/)[矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/)[转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/)[优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/)[分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/)[天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/)[深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/)[积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/)[python](https://kexue.fm/tag/python/)[力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/)[无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/)[扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)[几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/)[节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/)[生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/)[文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/)[数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

*   [学会提问的BERT：端到端地从篇章中构建问答对](https://kexue.fm/archives/7630)
*   [重新写了之前的新词发现算法：更快更好的新词发现](https://kexue.fm/archives/6920)
*   [《方程与宇宙》:活力积分和开普勒方程(二)](https://kexue.fm/archives/561)
*   [首次报名参加天文竞赛，期待中...](https://kexue.fm/archives/277)
*   [数学魔术——漂亮的近似](https://kexue.fm/archives/654)
*   [从采样看优化：可导优化与不可导优化的统一视角](https://kexue.fm/archives/7521)
*   [获取并处理中文维基百科语料](https://kexue.fm/archives/4176)
*   [美国3名科学家获诺贝尔生理学或医学奖](https://kexue.fm/archives/150)
*   [封闭曲线所围成的面积：一个新技巧](https://kexue.fm/archives/3441)
*   [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)

### 最近评论

*   [大力](https://kexue.fm/archives/9119/comment-page-13#comment-28682): 你好，苏老师请问（14）式子中β2tβ¯2tEε∼N(0,I)[∥∥∥ε−β¯tβtϵθ(α¯...
*   [苏剑林](https://kexue.fm/archives/11340/comment-page-1#comment-28680): 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上来讲，整体乘一个大数字，是不改变...
*   [苏剑林](https://kexue.fm/archives/9257/comment-page-4#comment-28679): 可以这样理解：t t 时刻的 x t x t，和 t−1 t−1 时刻的$\bold...
*   [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-28678): 没有太多技巧了，就是直接代入然后根据$\bar{\alpha}_t,\bar{\beta}_t...
*   [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28677): 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那Wropeq和Wropekt又是...
*   [Zhan-Wang Mao](https://kexue.fm/archives/9257/comment-page-4#comment-28676): 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和 t t 有关？不是在 x t x t 处关于 x x 的...
*   [yzlnew](https://kexue.fm/archives/11340/comment-page-1#comment-28675): 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和训练，并且也易于量化。
*   [Henry](https://kexue.fm/archives/9181/comment-page-5#comment-28673): 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？
*   [szsheep](https://kexue.fm/archives/9119/comment-page-13#comment-28672): 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度入手，没想到作者完全用另外一种方...
*   [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28671): 对于目前的MLA算法softmax(X×WQ×WukT×CjT)×Cj×Wuv来说其实X,WQ...

### 友情链接

*   [Cool Papers](https://papers.cool/)
*   [数学研发](https://bbs.emath.ac.cn/)
*   [Seatop](http://www.seatop.com.cn/)
*   [Xiaoxia](https://xiaoxia.org/)
*   [积分表-网络版](https://kexue.fm/sci/integral/index.html)
*   [丝路博傲](http://blog.dvxj.com/)
*   [数学之家](http://www.2math.cn/)
*   [有趣天文奇观](http://interesting-sky.china-vo.org/)
*   [TwistedW](http://www.twistedwg.com/)
*   [godweiyang](https://godweiyang.com/)
*   [AI柠檬](https://blog.ailemon.net/)
*   [王登科-DK博客](https://greatdk.com/)
*   [ESON](https://blog.eson.org/)
*   [枫之羽](https://fzhiy.net/)
*   [Mathor's blog](https://wmathor.com/)
*   [coding-zuo](https://coding-zuo.github.io/)
*   [博科园](https://www.bokeyuan.net/)
*   [孔皮皮的博客](https://www.kppkkp.top/)
*   [运鹏的博客](https://yunpengtai.top/)
*   [jiming.site](https://jiming.site/)
*   [OmegaXYZ](https://www.omegaxyz.com/)
*   [EAI猩球](https://www.robotech.ink/)
*   [文举的博客](https://liwenju0.com/)
*   [用代码打点酱油](https://bruceyuan.com/)
*   [Zhang's blog](https://armcvai.cn/)
*   [申请链接](https://kexue.fm/links.html)

[![Image 15: 署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。 

 © 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。
