# NLP问题特征表达基础 - 语言模型（Language Model）发展演化历程讨论 - 郑瀚 - 博客园
- URL: https://www.cnblogs.com/LittleHann/p/7200618.html
- Added At: 2025-10-25 08:57:05
- [Link To Text](2025-10-25-nlp问题特征表达基础---语言模型（language-model）发展演化历程讨论---郑瀚---博客园_raw.md)

## TL;DR
本文概述了NLP领域，从基础概念到高级模型，包括语言模型、统计模型、神经网络模型、词向量、GloVe、ELMo、GPT、BERT和GPT-2等。

## Summary
1. **NLP问题简介**：
    - **NLP内涵**：NLP旨在从文本数据中复原人们的感知世界，包括文本表示、词汇相关性分析、主题模型、意见情感分析等。
    - **文本表示重要性**：文本表示是NLP中核心地位，是其他模型建构的基础。

2. **语言模型（Language Model）简介**：
    - **语言模型定义**：语言模型用于判断语句是否合理。
    - **发展历史**：包括专家语法规则模型、统计语言模型和神经网络语言模型。

3. **统计语言模型（SLM）**：
    - **基本公式**：通过统计计算给定文本序列的概率分布。
    - **马尔科夫假设**：当前词出现的概率仅依赖前 n−1 个词。
    - **n-gram模型**：将文本进行大小为N的滑动窗口操作，形成长度为N的短语子序列。
    - **平滑方法**：如拉普拉斯平滑、加性平滑和古德图灵平滑。

4. **LSA（潜在语义分析）**：
    - **解决歧义与多义问题**：通过分析文章挖掘潜在语义，减少歧义和多义现象。

5. **神经网络语言模型（NNLM）**：
    - **问题**：传统统计语言模型存在维度灾难和数据稀疏问题。
    - **分布式表征**：使用连续变量（词向量）来表示单词，解决维度爆炸问题。
    - **Bengio NIPS’03 Model**：使用前馈神经网络表示语言模型，引入词向量矩阵层。

6. **RNN长序列语言模型**：
    - **局限**：传统模型难以捕捉长距离依赖关系。
    - **Recurrent Neural Networks**：通过递归神经网络建模序列数据。

7. **word2vec语言模型**：
    - **CBOW模型**：输入词的上下文预测中间词。
    - **Skip-gram模型**：输入特定词的词向量预测上下文词。
    - **Hierarchical Softmax**：通过二叉树优化softmax计算。
    - **Negative Sampling**：减少计算量，提高训练效率。

8. **GloVe（Global Vectors for Word Representation）**：
    - **基于全局词频统计**：使用词共现矩阵来提取和表征语料的词法和语义。
    - **词向量相似性推导**：通过词共现矩阵和词向量之间的近似关系来推导词向量。

9. **ELMo语言模型**：
    - **解决多义词问题**：通过预训练和微调来捕捉上下文信息。
    - **网络结构**：包括输入层、双向双层LSTM网络和线性组合层。

10. **GPT（Generative Pre-Training）**：
    - **基于多层transformer的单向语言模型**：利用语言模型的特性在海量语料库上进行预训练。

11. **BERT（Bidirectional Encoder Representations from Transformers）**：
    - **改进GPT**：使用transformer结构，融合上下文信息。

12. **GPT-2**：
    - **优化点**：使用更高质量的数据，增加模型复杂度，强调无监督和多任务学习特性。
