# OpenAI o1模型的前世今生
- URL: https://mp.weixin.qq.com/s/OCgbffOPrZ5kzFKisSUC9Q
- Added At: 2024-10-31 07:46:37
- [Link To Text](2024-10-31-openai-o1模型的前世今生_raw.md)

## TL;DR
OpenAI o1模型是具备强大复杂推理能力的语言模型，尤其在STEM领域超越人类博士水平。通过思维链技术和强化学习提升推理能力，未来将助力GPT-6发展，标志LLM进入“后训练”时代。

## Summary
1. **OpenAI o1模型简介**：OpenAI o1模型是OpenAI推出的具有强大复杂推理能力的语言模型，其在数学、代码等STEM领域表现出色，甚至超越人类博士水平。

2. **思维系统**：
   - **System 1**：快速、无意识的思考模式，类似于直觉反应。
   - **System 2**：缓慢、需要集中注意力的思考模式，类似于逻辑推理。

3. **数理推断问题**：LLM在数理推断问题上表现不佳，因为它们基于概率和感知，而非计算和推理。

4. **思维链（CoT）**：通过让LLM显式输出中间推理步骤，增强其算数、常识和推理能力。

5. **OpenAI o1的特点**：
   - 强大的复杂推理能力，尤其在STEM领域。
   - 推理能力超越人类博士水平。
   - 在竞争性编程、数学奥林匹克等竞赛中取得优异成绩。
   - 具备视觉感知功能，在多模态任务上表现突出。

6. **OpenAI o1的技术路线**：
   - **预训练阶段**：使用大量代码、数学等STEM领域语料以及CoT语料进行训练。
   - **后训练阶段**：进行RLHF、Self-Play RL和Safety训练，增强模型的对话能力和推理能力。
   - **推理阶段**：生成CoT并进行总结，输出最终答案。

7. **CoT数据构造**：
   - **人工标注**：成本高，但准确。
   - **合成CoT方法**：如STaR方法，利用LLM已有的推理能力，迭代式Bootstrap模型产生合理推理过程。

8. **后训练阶段的强化学习**：
   - **Self-Play**：类似于AlphaGo的自我博弈，通过Generator和Verifier的对抗来提升模型性能。
   - **PRM（过程监督奖励模型）**：提供每一步推理的反馈，比ORM（结果监督奖励模型）更有效。
   - **Critic Model**：训练CriticGPT作为Verifier，在代码等方面比人类评估者更胜一筹。

9. **推理阶段的CoT生成**：o1在推理阶段首先生成CoT然后进行Summary输出给用户，CoT过程长度可控。

10. **o1的意义**：o1代表了LLM在推理能力上的重大飞跃，其生成的大量数据将被用于训练下一代GPT-6模型，进一步提升模型的性能和思考模式。

11. **o1的复现**：目前大多数复现工作只是在Inference阶段做一些CoT设置，与o1的实际技术路线相差甚远。

12. **o1的未来**：o1的推出标志着LLM进入“后训练”时代，未来LLM将更加注重推理能力和思考模式的提升，性能将远超人类水平。
