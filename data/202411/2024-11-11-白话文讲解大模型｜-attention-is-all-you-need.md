# 白话文讲解大模型｜ Attention is all you need
- URL: https://mp.weixin.qq.com/s/_N-xXOx_XvdeWr1lls39sw
- Added At: 2024-11-11 11:51:04
- [Link To Text](2024-11-11-白话文讲解大模型｜-attention-is-all-you-need_raw.md)

## TL;DR
《Attention is all you need》：2017年Google、多伦多大学提出，基于注意力机制的Transformer架构，推动NLP新时代发展，核心为自注意力机制和Transformer模型，应用于文本生成、问答等。

## Summary
1. **论文概述**：
   - **论文名称**：《Attention is all you need》
   - **发布时间**：2017年6月12日
   - **发布单位**：Google、多伦多大学
   - **摘要**：提出基于注意力机制的Transformer架构，是LLM的始祖，推动NLP新时代发展。
   - **核心内容**：Transformer架构完全基于注意力机制，无需循环和卷积神经网络。

2. **LLM浅析**：
   - **LLM功能**：预测下一个可能出现的词汇（Token），应用于文本生成、问答系统、翻译、文本摘要、对话系统等。
   - **Token**：
     - **定义**：文本的基本单位，可以是词、标点、数字或单词的一部分。
     - **示例**：例如，"Strawberry"被分为"Str-aw-berry"三个token。
     - **处理**：LLM使用分词器将文本转换为Token列表，每个Token都有一个唯一标识符。
   - **模型训练**：
     - **任务**：预测句子中的下一个词。
     - **示例**：使用简单的词汇表和训练数据，构建概率矩阵预测下一个词。
     - **问题**：马尔可夫链方法在处理大规模上下文窗口时存在可扩展性问题。
   - **上下文窗口**：
     - **局限**：仅依赖最后一个Token预测，上下文窗口小。
     - **改进**：增加上下文窗口大小，但概率表大小呈指数级增长。
   - **神经网络**：
     - **作用**：更高效地预测下一个Token。
     - **训练**：涉及大量数学运算，如前向传播和反向传播。
   - **Transformer**：
     - **特点**：注意力机制，允许模型动态关注序列的不同部分。
     - **应用**：机器翻译和语言生成任务。

3. **Transformer输入**：
   - **组成**：单词Embedding和位置Embedding相加。
   - **目的**：提供单词的语义信息和位置信息。

4. **Self-Attention（自注意力机制）**：
   - **简介**：允许模型关注序列中的不同部分，适用于处理长文本。
   - **工作流程**：转换、计算注意力分数、归一化、加权求和。
   - **公式**：展示转换、注意力分数计算、归一化、加权求和的公式。
   - **Multi-Head Attention**：
     - **作用**：增强模型的表达能力，从多个角度综合考虑信息。
     - **结构**：并行运行多个Self-Attention层。

5. **Add & Norm 和 Feed Forward**：
   - **Add & Norm**：
     - **残差连接**：防止训练过程中信息丢失。
     - **层归一化**：让数据更稳定，加快训练速度。
   - **Feed Forward**：
     - **作用**：增加非线性，让模型更灵活。
     - **结构**：两层全连接网络。

6. **Decoder结构**：
   - **特点**：与Encoder block相似，但存在一些区别。
   - **操作**：包含两个Multi-Head Attention层，采用Masked操作，使用编码信息矩阵C和上一个Decoder block的输出计算。

7. **附录**：
   - **代码示例**：展示如何导入库和进行相关操作。
   - **讨论**：欢迎留言参与讨论。
