Title: ğŸŒŸã€Šä»ä¸€åˆ°äºŒï¼šåŸºäºTraeçš„å¤šæ™ºèƒ½ä½“åˆä½œPromptè‡ªåŠ¨ç”Ÿæˆè¿­ä»£æŒ‡å—ã€‹#VibeCoding #Trae

URL Source: https://juejin.cn/post/7488013596523118601

Published Time: 2025-04-01T08:28:53+00:00

Markdown Content:
èƒŒæ™¯
--

Multi-agentç»“åˆMCPå·²ç»æˆä¸ºéå¸¸æ˜ç¡®çš„æ–°ä¸€è½®å‘å±•è¶‹åŠ¿ï¼Œä½†æ—¥å¸¸å·¥ä½œå‘ç°ï¼Œå¾ˆå¤šåˆšæ¥è§¦çš„å°ä¼™ä¼´è¿˜åœ¨å—â€œå¦‚ä½•ç¼–å†™ä¸€ä¸ªèƒ½å®Œæˆè‡ªå·±é¢„æœŸéœ€æ±‚çš„promptâ€è¿™ä¸€éœ€æ±‚è€Œæ„Ÿåˆ°å›°æ‰°ã€‚é‚£ä¹ˆç»“åˆMulti-agentï¼Œæˆ‘ä»¬ä¸å¦¨è®¾è®¡å’Œå®ç°ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åˆä½œæ¥è‡ªåŠ¨åŒ–å®Œæˆpromptè‡ªåŠ¨è¿­ä»£ç”Ÿæˆçš„ç³»ç»Ÿï¼ŒLet's do it!

æ¡†æ¶å’ŒLLMéƒ¨ç½²æ–¹å¼é€‰å‹
------------

ä¸åŒäºæˆ‘ä»¬ä¹‹å‰é‡‡ç”¨[langroid](https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Flangroid%2Flangroid "https://github.com/langroid/langroid")çš„é€‰å‹ï¼Œæœ¬æœŸæˆ‘ä»¬å°†é‡‡ç”¨langchainï¼Œè¿™é‡Œåšå‡ºå˜æ›´çš„åŸå› ä¸»è¦åœ¨äºlangchainä¼¼ä¹å¤§å®¶ä½¿ç”¨çš„æ›´ä¸ºå¹¿æ³›ï¼Œä½†åœ¨æ˜“ç”¨æ€§ä¸Šæœ¬æ–‡ä»ç„¶æ¨èlangroidï¼Œæ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥å‚çœ‹æˆ‘ä»¬å‰ä¸€ç³»åˆ—ä¸­çš„å†…å®¹ã€‚

*   Multi-Agent: [langchainï¼ˆpythonï¼‰](https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fintroduction%2F "https://python.langchain.com/docs/introduction/")
    
*   LLMéƒ¨ç½²ï¼ˆåŒå‰ä¸€ç³»åˆ—ï¼‰ï¼š
    
    *   éœ€æ‰€é€‰å®šçš„Multi-agentæ¡†æ¶æ”¯æŒï¼Œç†è®ºä¸Šæµè¡Œçš„æ¡†æ¶ç›®å‰éƒ½æ”¯æŒç±»openaiçš„æ¥å£æ–¹å¼æ¥æ¥å…¥è¿œç¨‹å¤§æ¨¡å‹ï¼Œæˆ–è€…æ˜¯æœ¬åœ°éƒ¨ç½²çš„LLMæ¨¡å‹ï¼Œç”šè‡³æ˜¯ç±»ä¼¼äºcozeè¿™ç§ç›´æ¥é€‰å–å³å¯ã€‚
    *   è€ƒè™‘åˆ°æœ¬åœ°éƒ¨ç½²æ¨¡å‹æ— åˆè§„é£é™©ï¼Œqueryæ— é¢å¤–é™åˆ¶ï¼ˆå¦‚é€Ÿç‡é™åˆ¶ï¼‰ï¼Œä¸”å…è´¹ï¼Œæœ¬é¡¹ç›®ä»¥[Ollama](https://link.juejin.cn/?target=https%3A%2F%2Follama.com%2F "https://ollama.com/")éƒ¨ç½²æœ¬åœ°æ¨¡å‹ä¸ºä¾‹ï¼Œè¿›è¡Œå±•ç¤ºã€‚
    *   [ollmaä½¿ç”¨](https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Follama%2Follama%3Ftab%3Dreadme-ov-file "https://github.com/ollama/ollama?tab=readme-ov-file") & [ollmaæä¾›çš„æ¨¡å‹åˆ—è¡¨](https://link.juejin.cn/?target=https%3A%2F%2Follama.com%2Flibrary "https://ollama.com/library")
    *   ![Image 1](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/4d5705c99dae4b56aec0dd5b5f0d1fbb~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5p2O5biFMTQ5Njk4MjQxOTE2Nw==:q75.awebp?rk3s=f64ab15b&x-expires=1744101205&x-signature=P20fAu2JEwbJxy1Ptk3FpWsRa5s%3D)

æ–¹æ¡ˆè®¾è®¡
----

æš‚æ—¶æ— æ³•åœ¨é£ä¹¦æ–‡æ¡£å¤–å±•ç¤ºæ­¤å†…å®¹

###### workflow

*   ç”¨æˆ·éœ€æ±‚
    
    *   åŒ…æ‹¬ç”¨æˆ·æƒ³è¦è®¾è®¡ä¸€ä¸ªä»€ä¹ˆæ ·çš„prompt
    *   é’ˆå¯¹è¿™ä¸ªæƒ³è®¾è®¡çš„promptï¼Œç”¨æˆ·éœ€æä¾›ä¸€ä¸ªé¢„æœŸè¾“å…¥å’Œç›¸åº”çš„é¢„æœŸè¾“å‡º
*   Main Agent
    
    *   æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œé¢„æœŸè¾“å…¥è¾“å‡ºï¼Œç”Ÿæˆåˆå§‹Prompt
    *   æ ¹æ®åé¦ˆè¿­ä»£Promptï¼Œå½“è¶…å‡ºæœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–å½“å‰è°ƒæ•´åçš„promptå·²è¾¾æˆéœ€æ±‚ï¼Œåˆ™è¾“å‡ºæœ€ç»ˆprompt
*   Validator Agent
    
    *   è¯»å–promptå¹¶åŠ è½½ï¼Œä»¥é¢„æœŸè¾“å…¥ä¸ºè¾“å…¥ï¼Œå¾—åˆ°å®é™…è¾“å‡ºï¼Œå¯¹æ¯”é¢„æœŸè¾“å‡ºï¼Œè¾“å‡ºOutputå·®å¼‚
*   Analysis Agent
    
    *   åˆ†æOutputå·®å¼‚ï¼Œå‘Main Agentæå‡ºé’ˆå¯¹æ€§çš„åé¦ˆè°ƒæ•´æ„è§

ä»å¤´å¼€å§‹
----

![Image 2](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/e647eb35e9b9494395c99a73f3bc44a5~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5p2O5biFMTQ5Njk4MjQxOTE2Nw==:q75.awebp?rk3s=f64ab15b&x-expires=1744101205&x-signature=32%2FOxf8OI0as1VZscn4M4XfWm2M%3D)

###### ç¯å¢ƒå‡†å¤‡

1.  éƒ¨ç½²æœ¬åœ°LLMï¼š
    
    1.  ä¸‹è½½å®‰è£…[ollama](https://link.juejin.cn/?target=https%3A%2F%2Follama.com%2F "https://ollama.com/")
    2.  ![Image 3](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/9f27a9e363d84a6099b658a348d8c163~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5p2O5biFMTQ5Njk4MjQxOTE2Nw==:q75.awebp?rk3s=f64ab15b&x-expires=1744101205&x-signature=fDizfETmLh3izvqIG8oaUEO%2FZk4%3D)
    3.  pullä¸‹æ¥`deepseek-r1:14b`æ¨¡å‹ï¼Œä½ å¯ä»¥[æŒ‰éœ€é€‰æ‹©](https://link.juejin.cn/?target=https%3A%2F%2Follama.com%2Flibrary "https://ollama.com/library")
    4.  ```
          ollama pull deepseek-r1:14b
        ```
        
    
2.  å®‰è£…langchainï¼Œlanggraphç­‰ç›¸å…³ä¾èµ–
    

```
pip3 install -r requirements.txt
```

requirements.txtä¸ºå½“å‰é¡¹ç›®ä¾èµ–çš„python libä¿¡æ¯ï¼Œå¦‚ä¸‹ï¼š

```
langchain>=0.1.0
langgraph>=0.0.10
langchain-community>=0.0.10
langchain-core>=0.1.0
typing-extensions>=4.8.0
typing>=3.7.4
pydantic>=2.5.0
ollama>=0.1.5
python-dotenv>=1.0.0
```

###### Coding

IDEï¼š[Trae](https://www.trae.ai/ "https://www.trae.ai/") æˆ– [Trae CN](https://www.trae.com.cn/ "https://www.trae.com.cn/")

1.**åˆ›å»ºå·¥ä½œæµ**ï¼šé¦–å…ˆè®©æˆ‘ä»¬æŒ‰ç…§ä¸Šè¿°æ–¹æ¡ˆè®¾è®¡é‡Œçš„workflowæ¥åˆ›å»ºä¸€ä¸ªå·¥ä½œæµ

```
def create_workflow() -> Graph:
    workflow = Graph()

    # å®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
    workflow.add_node("generate_prompt", generate_prompt)
    workflow.add_node("validate_prompt", validate_prompt)
    workflow.add_node("generate_feedback", generate_feedback)
    workflow.add_node("check_completion", check_completion)

    # å®šä¹‰å·¥ä½œæµè¾¹å’Œæ¡ä»¶
    workflow.add_edge("generate_prompt", "validate_prompt")
    workflow.add_edge("validate_prompt", "check_completion")
    workflow.add_conditional_edges(
        "check_completion",
        lambda x: x[0],
        {
            "continue": "generate_feedback", 
            "end": END
        }
    )
    workflow.add_edge("generate_feedback", "generate_prompt")

    # è®¾ç½®å·¥ä½œæµå…¥å£
    workflow.set_entry_point("generate_prompt")

    return workflow.compile()
```

2.**å®ç°å·¥ä½œæµä¸­çš„å…³é”®èŠ‚ç‚¹**ï¼š

*   generate\_prompt(å¯¹åº”MainAgent)ï¼š

```
def generate_prompt(state: WorkflowState) -> WorkflowState:
    print("\nğŸ¯ è¿›å…¥ç”Ÿæˆæç¤ºè¯é˜¶æ®µ...")
    main_agent = MainAgent()
    if len(state["current_prompt"]) == 0:
        # å°†æ‰€æœ‰ç¤ºä¾‹ä¼ é€’ç»™ä¸»æ™ºèƒ½ä½“
        state["current_prompt"] = main_agent.generate_initial_prompt(
            state["user_goal"],
            state["examples"]
        )
    else:
        state["current_prompt"] = main_agent.optimize_prompt(
            state["feedback"],
            state["current_prompt"]
        )
    return state
```

*   validate\_promptï¼ˆå¯¹åº”validatorAgentï¼‰ï¼š

```
def validate_prompt(state: WorkflowState) -> WorkflowState:
    print("\nğŸ” è¿›å…¥éªŒè¯æç¤ºè¯é˜¶æ®µ...")
    validator = ValidatorAgent()
    validation_result = validator.validate_output(state["current_prompt"], state["examples"])
    
    # ç¡®ä¿validation_resultåŒ…å«is_acceptableå­—æ®µ
    if isinstance(validation_result, dict):
        validation_result.setdefault("is_acceptable", False)
    else:
        validation_result = {"is_acceptable": False, "individual_results": [], "overall_score": 0.0}
    
    state["validation_result"] = validation_result
    return state
```

*   generate\_feedbackï¼ˆå¯¹åº”AnalysisAgentï¼‰ï¼š

```
def generate_feedback(state: WorkflowState) -> WorkflowState:
    print("\nğŸ’­ è¿›å…¥ç”Ÿæˆåé¦ˆé˜¶æ®µ...")
    feedback_agent = FeedbackAgent()
    
    # å¤„ç†check_completionè¿”å›çš„å…ƒç»„çŠ¶æ€
    if isinstance(state, tuple):
        _, state = state
    
    # ç¡®ä¿stateæ˜¯å­—å…¸ç±»å‹å¹¶åŒ…å«æ‰€æœ‰å¿…éœ€é”®
    if not isinstance(state, dict) or not all(key in state for key in ["current_prompt", "validation_result", "feedback", "user_goal", "examples", "iteration_count", "max_retries", "is_complete"]):
        print(f"è­¦å‘Šï¼šçŠ¶æ€å¯¹è±¡ä¸å®Œæ•´æˆ–ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œå½“å‰çŠ¶æ€å†…å®¹ä¸º: {str(state)}")
        if hasattr(state, '__dict__'):
            state = vars(state)
        else:
            state = {
                "feedback": getattr(state, 'feedback', ""), 
                "current_prompt": getattr(state, 'current_prompt', ""), 
                "validation_result": getattr(state, 'validation_result', {}), 
                "user_goal": getattr(state, 'user_goal', ""), 
                "examples": getattr(state, 'examples', []), 
                "iteration_count": getattr(state, 'iteration_count', 0), 
                "max_retries": getattr(state, 'max_retries', 3), 
                "is_complete": getattr(state, 'is_complete', False)
            }
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨
        for key in ["current_prompt", "validation_result", "feedback", "user_goal", "examples", "iteration_count", "max_retries", "is_complete"]:
            if key not in state:
                state[key] = "" if key in ["current_prompt", "feedback", "user_goal"] else ([] if key == "examples" else ({} if key == "validation_result" else (0 if key == "iteration_count" else (3 if key == "max_retries" else False))))
    
    try:
        # æ£€æŸ¥å¿…è¦çš„é”®æ˜¯å¦å­˜åœ¨
        if "current_prompt" not in state or "validation_result" not in state:
            print(f"é”™è¯¯ï¼šçŠ¶æ€å¯¹è±¡ç¼ºå°‘å¿…è¦çš„é”®")
            state["feedback"] = ""
            return state
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å·®å¼‚
        has_differences = any(
            len(result.get('analysis_result', {}).get('differences', [])) > 0
            for result in state["validation_result"].get("individual_results", [])
        )
        
        feedback = feedback_agent.generate_feedback(
            state["current_prompt"],
            state["validation_result"]
        )
            
        if feedback == "NO_DIFFERENCES":
            state["validation_result"]["is_acceptable"] = True
            state["is_complete"] = True
            return state
        
        state["feedback"] = feedback
    except Exception as e:
        print(f"é”™è¯¯ï¼šç”Ÿæˆåé¦ˆæ—¶å‘ç”Ÿå¼‚å¸¸ - {str(e)}")
        state["feedback"] = ""
    return state
```

*   check\_completion(ç”¨æ¥æ£€æŸ¥è¿­ä»£åçš„promptæ˜¯ä¸æ˜¯okï¼Œæˆ–è€…è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°äº†)ï¼š

```
def check_completion(state: WorkflowState) -> tuple[str, WorkflowState]:
    print("\nâœ¨ è¿›å…¥å®Œæˆæ£€æŸ¥é˜¶æ®µ...")
    state["iteration_count"] += 1
    
    # æ£€æŸ¥éªŒè¯ç»“æœæ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
    if not state.get("validation_result") or not isinstance(state["validation_result"], dict):
        print("è­¦å‘Šï¼šæ— æ•ˆçš„éªŒè¯ç»“æœ")
        return ("continue", state)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·®å¼‚
    has_differences = any(
        len(result.get('analysis_result', {}).get('differences', [])) > 0
        for result in state["validation_result"].get("individual_results", [])
    )
    
    if state["validation_result"]["is_acceptable"] and not has_differences:
        state["is_complete"] = True
        return ("end", state)
    
    if state["iteration_count"] >= state["max_retries"]:
        state["is_complete"] = True
        return ("end", state)
    
    return ("continue", state)
```

3.å®ç°å…³é”®Agentå¤„ç†é€»è¾‘ï¼š

*   Main Agentï¼š

```
from langchain.agents import AgentExecutor, BaseSingleActionAgent
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models.ollama import ChatOllama
from langchain.schema import SystemMessage, HumanMessage
from typing import Dict, List, Any, Optional
from langchain.schema import AgentAction

class MainAgent(BaseSingleActionAgent):
    llm: ChatOllama = None
    initial_template: str = None
    optimization_template: str = None
    prompt_template: Optional[ChatPromptTemplate] = None
    iteration_history: List[Dict[str, Any]] = None

    def __init__(self):
        super().__init__()
        # æ¨èæ¢ä¸ªå°ä¸€ç‚¹sizeå‚æ•°é‡çš„æ¨¡å‹ï¼Œå®æµ‹ä¸‹æ¥32Gå†…å­˜çš„mbpï¼Œè·‘èµ·æ¥è¿˜æ˜¯å¡
        self.llm = ChatOllama(model='gemma3:27b')
        self.iteration_history = []
        self.prompt_template = None  # å°†åœ¨å…·ä½“æ–¹æ³•ä¸­æ ¹æ®éœ€è¦é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ¨¡æ¿
        self.initial_template = """æ‚¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯å·¥ç¨‹å¸ˆï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ç”Ÿæˆsystem promptï¼š
            1. ç›®æ ‡åˆ†æï¼š
               - æ·±å…¥åˆ†æç”¨æˆ·ç›®æ ‡ï¼š{user_goal}
               - è¯†åˆ«å…³é”®éœ€æ±‚å’Œçº¦æŸæ¡ä»¶
               - ç¡®ä¿ç”Ÿæˆçš„promptç›´æ¥æœåŠ¡äºç›®æ ‡
               - è€ƒè™‘ç›®æ ‡çš„è¾¹ç•Œåœºæ™¯å’Œå¼‚å¸¸æƒ…å†µ
               - è¯„ä¼°ç›®æ ‡çš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§
            
            2. è¯­ä¹‰åˆ†æï¼š
               - ç¡®ä¿promptè¯­ä¹‰å®Œæ•´æ€§å’Œæ¸…æ™°åº¦
               - åˆ†æä¸Šä¸‹æ–‡å…³è”æ€§å’Œä¾èµ–å…³ç³»
               - è¯†åˆ«å…³é”®æ¦‚å¿µå’Œæœ¯è¯­
               - è€ƒè™‘é¢†åŸŸç‰¹å®šçš„ä¸“ä¸šæœ¯è¯­å’Œæœ€ä½³å®è·µ
               - ç¡®ä¿æŒ‡ä»¤çš„é€»è¾‘æ€§å’Œè¿è´¯æ€§
            
            3. è¾“å…¥è¾“å‡ºç¤ºä¾‹å¤„ç†ï¼š
               - æ·±å…¥åˆ†æç¤ºä¾‹è¾“å…¥ï¼š{example_input}
               - åˆ†ææœŸæœ›è¾“å‡ºï¼š{example_output}
               - æå–è¾“å…¥è¾“å‡ºçš„å…³é”®ç‰¹å¾å’Œæ¨¡å¼
               - è¯†åˆ«è¾“å…¥è¾“å‡ºçš„è¾¹ç•Œæ¡ä»¶å’Œç‰¹æ®Šæƒ…å†µ
               - ç¡®ä¿promptèƒ½å¤Ÿå¤„ç†ç±»ä¼¼çš„è¾“å…¥åœºæ™¯å¹¶ç”Ÿæˆç¬¦åˆé¢„æœŸçš„è¾“å‡ºæ ¼å¼
            
            4. è´¨é‡æ ‡å‡†ï¼š
               - ç¡®ä¿promptç®€æ´æ˜ç¡®ç³»ç»Ÿä¸”ç»“æ„åŒ–ï¼Œé¿å…å†—ä½™
               - ä½¿ç”¨ç²¾ç¡®çš„æŒ‡ä»¤å’Œçº¦æŸæ¡ä»¶
               - ä¿æŒä¸€è‡´çš„è¯­è¨€é£æ ¼å’Œæ ¼å¼
               - ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸ç¤ºä¾‹è¾“å‡ºä¿æŒä¸€è‡´
               - è€ƒè™‘promptçš„å¯æµ‹è¯•æ€§å’Œå¯ç»´æŠ¤æ€§
               - è¯„ä¼°promptçš„é²æ£’æ€§å’Œå®¹é”™æ€§
            
            è¯·åŸºäºä»¥ä¸Šè¦æ±‚ï¼Œç›´æ¥è¾“å‡ºä¸€ä¸ªä¸“ä¸šã€å…¨é¢ä¸”é«˜è´¨é‡çš„system promptï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§å†…å®¹æˆ–åˆ†æè¿‡ç¨‹ã€‚
            æ³¨æ„ï¼šç”Ÿæˆçš„promptåº”å½“å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ã€é²æ£’æ€§å’Œä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§è¾¹ç•Œåœºæ™¯å’Œå¼‚å¸¸æƒ…å†µã€‚"""
            
        self.optimization_template = """æ‚¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯å·¥ç¨‹å¸ˆï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–system promptï¼š
            å½“å‰Promptï¼š
            {example_input}            

            é’ˆå¯¹å½“å‰Promptçš„åé¦ˆï¼š
            {feedback}
            
            è¯·ä½ æŒ‰ç…§å¦‚ä¸‹è¦æ±‚åˆ†æåé¦ˆï¼Œä¼˜åŒ–å½“å‰Promptã€‚
            1. å½“å‰Promptåˆ†æï¼š
               - æ·±å…¥åˆ†æå½“å‰prompt
               - è¯†åˆ«ç°æœ‰promptçš„ä¼˜ç‚¹å’Œä¸è¶³
               - è¯„ä¼°promptçš„ç»“æ„å’Œç»„ç»‡æ–¹å¼
               - æ£€æŸ¥promptçš„è¯­ä¹‰å®Œæ•´æ€§å’Œæ¸…æ™°åº¦
            
            2. åé¦ˆåˆ†æï¼š
               - ä»”ç»†åˆ†æä¼˜åŒ–åé¦ˆ
               - è¯†åˆ«éœ€è¦æ”¹è¿›çš„å…³é”®ç‚¹
               - ç¡®å®šä¼˜åŒ–çš„ä¼˜å…ˆçº§å’Œæ–¹å‘
               - è€ƒè™‘åé¦ˆçš„åˆç†æ€§å’Œå¯è¡Œæ€§
            
            3. ä¼˜åŒ–ç­–ç•¥ï¼š
               - ä¿æŒpromptçš„æ ¸å¿ƒåŠŸèƒ½å’Œä¼˜ç‚¹
               - é’ˆå¯¹æ€§åœ°è§£å†³åé¦ˆä¸­æå‡ºçš„é—®é¢˜
               - ç¡®ä¿ä¼˜åŒ–åçš„promptæ›´åŠ ä¸“ä¸šå’Œé«˜æ•ˆ
               - å¢å¼ºpromptçš„é²æ£’æ€§å’Œé€‚åº”æ€§
            
            4. è´¨é‡ä¿è¯ï¼š
               - ç¡®ä¿ä¼˜åŒ–åçš„promptç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†
               - ä¿æŒè¯­è¨€é£æ ¼çš„ä¸€è‡´æ€§å’Œä¸“ä¸šæ€§
               - éªŒè¯ä¼˜åŒ–æ˜¯å¦è§£å†³äº†åŸæœ‰é—®é¢˜
               - è¯„ä¼°ä¼˜åŒ–åçš„promptæ˜¯å¦æ›´æ˜“äºä½¿ç”¨å’Œç»´æŠ¤
            
            è¯·åŸºäºä»¥ä¸Šè¦æ±‚ï¼Œç›´æ¥è¾“å‡ºä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„ã€æ›´é«˜è´¨é‡çš„system promptï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§å†…å®¹æˆ–åˆ†æè¿‡ç¨‹ã€‚
            æ³¨æ„ï¼šä¼˜åŒ–åçš„promptåº”å½“ä¿æŒåŸæœ‰çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶æ˜¾è‘—æ”¹è¿›å·²è¯†åˆ«çš„é—®é¢˜ã€‚"""
            
        self.prompt_template = None  # å°†åœ¨å…·ä½“æ–¹æ³•ä¸­æ ¹æ®éœ€è¦é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ¨¡æ¿
        
    @property
    def input_keys(self):
        return ["user_goal", "example_input", "example_output", "feedback"]

    @property
    def tool_run_logging_kwargs(self) -> Dict[str, Any]:
        return {"handle_tool_error": True}

    def plan(self, inputs: Dict[str, str], intermediate_steps: List = None) -> List[Dict[str, Any]]:
        return self.aplan(inputs, intermediate_steps)

    def generate_initial_prompt(self, user_goal: str, examples: List[Dict[str, str]]) -> str:
        """ç”Ÿæˆåˆå§‹çš„system prompt"""
        print("\nğŸ¯ æ­£åœ¨ç”Ÿæˆåˆå§‹æç¤ºè¯...")
        # æ ¼å¼åŒ–æ‰€æœ‰ç¤ºä¾‹
        formatted_examples = []
        for i, example in enumerate(examples, 1):
            formatted_examples.append(f"ç¤ºä¾‹{i}ï¼š\nè¾“å…¥ï¼š{example['input']}\né¢„æœŸè¾“å‡ºï¼š{example['expected_output']}")
        
        examples_text = "\n\n".join(formatted_examples)
        
        self.prompt_template = ChatPromptTemplate.from_template(self.initial_template)
        messages = self.prompt_template.format(
            user_goal=user_goal,
            example_input=examples_text,
            example_output=""  # ä¸å†å•ç‹¬ä½¿ç”¨example_output
        )
        response = self.llm.invoke(messages)
        return response.content.strip()

    def aplan(self, inputs: Dict[str, str], intermediate_steps: List = None) -> List[Dict[str, Any]]:
        messages = self.prompt_template.format(
            user_goal=inputs["user_goal"],
            example_input=inputs["example_input"],
            example_output=inputs["example_output"],
            feedback=inputs["feedback"]
        )
        response = self.llm.invoke(messages)
        return [AgentAction(
            tool="prompt_optimization",
            tool_input={"optimized_prompt": response.content},
            log=f"Iteration {len(self.iteration_history)+1}"
        )]

    def optimize_prompt(self, feedback: str, current_prompt: str) -> str:
        """ä¼˜åŒ–å½“å‰çš„system prompt"""
        print("\nâœ¨ æ­£åœ¨æ ¹æ®åé¦ˆä¼˜åŒ–æç¤ºè¯...")
        self.prompt_template = ChatPromptTemplate.from_template(self.optimization_template)
        messages = self.prompt_template.format(
            example_input=current_prompt,
            feedback=feedback
        )
        response = self.llm.invoke(messages)
        return response.content.strip()
```

*   Validator agentï¼š

```
from langchain_community.chat_models.ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage
from typing import Dict
from analysis_agent import AnalysisAgent

class ValidatorAgent:
    def __init__(self):
        pass

    def validate_output(self, system_prompt: str, examples: list) -> Dict:
        """è°ƒç”¨æ¨¡å‹éªŒè¯å¤šä¸ªç¤ºä¾‹è¾“å‡ºå¹¶è®¡ç®—å·®å¼‚"""
        print("\nğŸ”„ æ­£åœ¨éªŒè¯æ¨¡å‹è¾“å‡º...")
        chat = ChatOllama(model='gemma3:27b', temperature=0.2)
        
        all_results = []
        for example in examples:            
            print("ğŸ“Œ éªŒè¯-è¾“å…¥å†…å®¹ï¼š")
            print(example['input'])            
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=example['input'])
            ]
            response = chat.invoke(messages)
            
            actual_output = response.content.strip()            
            print("\nğŸ” è¾“å‡ºå¯¹æ¯”ï¼š")
            print(f"é¢„æœŸè¾“å‡º: {example['expected_output']}")
            print(f"å®é™…è¾“å‡º: {actual_output}")
            
            # è°ƒç”¨åˆ†ææ™ºèƒ½ä½“åˆ†æå·®å¼‚
            analysis = AnalysisAgent().analyze_diff([{
                'actual_output': actual_output,
                'expected_output': example['expected_output']
            }])
            
            all_results.append({
                'input': example['input'],
                'actual_output': actual_output,
                'expected_output': example['expected_output'],
                'analysis_result': analysis
            })
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†ï¼ˆåŸºäºå·®å¼‚æ•°é‡å’Œè´¨é‡ï¼‰
        total_differences = sum(
            len(result['analysis_result']['differences'])
            for result in all_results
        )
        
        # å¦‚æœæ²¡æœ‰å·®å¼‚ï¼Œè¯„åˆ†ä¸º1ï¼›å¦åˆ™æ ¹æ®å·®å¼‚æ•°é‡è®¡ç®—ï¼ˆå·®å¼‚è¶Šå¤šåˆ†æ•°è¶Šä½ï¼‰
        overall_score = 1.0 if total_differences == 0 else 1.0 / (1 + total_differences)
        
        # åˆ¤æ–­æ˜¯å¦å¯æ¥å—ï¼ˆè¯„åˆ†å¤§äº0.7è§†ä¸ºå¯æ¥å—ï¼‰
        is_acceptable = overall_score > 0.7
        
        return {
            'is_acceptable': is_acceptable,
            'individual_results': all_results,
            'overall_score': overall_score
        }
```

*   Analysis agent:(ä¸Šè¿°validator agentä¸»è¦è°ƒç”¨analysis agentæ¥è¯†åˆ«å·®å¼‚)

```
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models.ollama import ChatOllama
from typing import Dict

class AnalysisAgent:
    llm: ChatOllama = None
    
    def __init__(self):
        self.llm = ChatOllama(model='deepseek-r1:14b')
        
    def analyze_diff(self, diff_reports: list) -> Dict:
        """ä½¿ç”¨ç»“æ„åŒ–promptè¿›è¡Œå·®å¼‚åˆ†æ"""
        print("\nğŸ“Š æ­£åœ¨åˆ†æè¾“å‡ºå·®å¼‚...")
        system_prompt = """æ‚¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¾“å‡ºå·®å¼‚åˆ†æä¸“å®¶ã€‚è¯·åˆ†ææ¨¡å‹å®é™…è¾“å‡ºå’Œé¢„æœŸè¾“å‡ºçš„å·®å¼‚ï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹ç»´åº¦ï¼š

        1. å†…å®¹å·®å¼‚ï¼š
           - é¢„æœŸè¾“å‡ºä¸­å­˜åœ¨ä½†å®é™…è¾“å‡ºç¼ºå¤±çš„å†…å®¹
           - å®é™…è¾“å‡ºä¸­å‡ºç°ä½†é¢„æœŸè¾“å‡ºæ²¡æœ‰çš„å†…å®¹
           - å†…å®¹çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
           - å…³é”®ä¿¡æ¯çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§

        2. æ ¼å¼å·®å¼‚ï¼š
           - è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸï¼ˆå¦‚JSONæ ¼å¼ã€åˆ—è¡¨æ ¼å¼ç­‰ï¼‰
           - è¾“å‡ºç»“æ„æ˜¯å¦ä¸€è‡´
           - æ ¼å¼è§„èŒƒæ€§å’Œæ ‡å‡†åŒ–ç¨‹åº¦

        è¯·ç›´æ¥æŒ‡å‡ºå…·ä½“çš„å·®å¼‚ç‚¹ï¼Œæ¯ä¸ªå·®å¼‚æ ‡æ³¨ç±»å‹ï¼ˆå†…å®¹ç¼ºå¤±/å†…å®¹å¤šä½™/æ ¼å¼ä¸ä¸€è‡´/è¯­ä¹‰åå·®ï¼‰ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§å†…å®¹ã€åˆ†æè¿‡ç¨‹æˆ–ä¸€è‡´çš„ç‚¹ã€‚
        å¦‚æœæ²¡æœ‰å·®å¼‚åˆ™è¾“å‡º:"æ— ä»»ä½•å·®å¼‚"ã€‚"""
        
        all_differences = []
        for i, diff_report in enumerate(diff_reports, 1):
            print(f"\nğŸ” åˆ†æç¤ºä¾‹ {i}ï¼š")
            
            # éªŒè¯è¾“å…¥æ ¼å¼
            if not isinstance(diff_report, dict) or 'actual_output' not in diff_report or 'expected_output' not in diff_report:
                print(f"è­¦å‘Šï¼šç¤ºä¾‹ {i} çš„å·®å¼‚æŠ¥å‘Šæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºåŒ…å«actual_outputå’Œexpected_outputçš„å­—å…¸")
                continue
                
            print("ğŸ“Œ å·®å¼‚æŠ¥å‘Šå†…å®¹ï¼š")
            print(f"å®é™…è¾“å‡º: {diff_report['actual_output']}")
            print(f"é¢„æœŸè¾“å‡º: {diff_report['expected_output']}")
            
            diff_content = f"å®é™…è¾“å‡º: {diff_report['actual_output']}\né¢„æœŸè¾“å‡º: {diff_report['expected_output']}"
            
            result = self.llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=f'\n{diff_content}')
            ])
            
            print("\nğŸ” åˆ†æç»“æœï¼š")
            print(result.content)
            
            if result and hasattr(result, 'content') and result.content:
                all_differences.append(result.content)
            
        return {
            'analysis_text': '\n'.join(all_differences) if all_differences else 'æ²¡æœ‰å‘ç°æœ‰æ•ˆå·®å¼‚',
            'differences': all_differences
        }
```

*   Feedback agentï¼š

```
from langchain_community.chat_models.ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage
from typing import Dict
from langchain_core.prompts import ChatPromptTemplate

class FeedbackAgent:
    optimization_history: list = None
    llm: ChatOllama = None
    
    def __init__(self):
        self.optimization_history = []
        self.llm = ChatOllama(model='deepseek-r1:14b')

    def generate_feedback(self, current_prompt: str, validation_result: Dict) -> str:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("\nğŸ” æ­£åœ¨åˆ†æéªŒè¯ç»“æœå¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        # æ·»åŠ ç±»å‹æ£€æŸ¥å’Œå®‰å…¨è®¿é—®é€»è¾‘
        if not isinstance(validation_result, dict) or 'individual_results' not in validation_result:
            print("é”™è¯¯ï¼šæ— æ•ˆçš„éªŒè¯ç»“æœæ ¼å¼")
            return ""
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å·®å¼‚
        has_differences = any(
            len(result.get('analysis_result', {}).get('differences', [])) > 0
            for result in validation_result['individual_results']
        )
        
        if not has_differences:
            print("âœ… æ— ä»»ä½•å·®å¼‚ï¼Œè¿”å›ç‰¹å®šå­—ç¬¦ä¸²")
            return "NO_DIFFERENCES"
            
        # ç”Ÿæˆæ¯ä¸ªç¤ºä¾‹çš„å·®å¼‚æ‘˜è¦
        example_summaries = []
        for i, result in enumerate(validation_result['individual_results']):
            if not isinstance(result, dict) or 'analysis_result' not in result:
                continue
                
            analysis_result = result.get('analysis_result', {})
            differences = analysis_result.get('differences', [])
            if differences:
                print(f"\nğŸ“Œ ç¤ºä¾‹ {i+1} å·®å¼‚è¯¦æƒ…ï¼š")
                for diff in differences:
                    if isinstance(diff, str):
                        print(f"- {diff}")
                    elif isinstance(diff, dict):
                        print(f"- {diff.get('description', '')}ï¼ˆç±»å‹ï¼š{diff.get('type', '')}ï¼‰")
                
                example_summaries.append(f"ç¤ºä¾‹ {i+1} åˆ†æï¼š\n" + 
                    "\n".join([f"- {diff}" if isinstance(diff, str) else 
                              f"- {diff['description']}ï¼ˆç±»å‹ï¼š{diff['type']}ï¼‰" 
                              for diff in differences])
                )
        
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content="""æ‚¨æ˜¯ä¸€ä¸ªæç¤ºè¯ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç»´åº¦æ·±å…¥åˆ†æå¹¶ç”Ÿæˆä¸“ä¸šçš„ä¼˜åŒ–å»ºè®®ï¼š
            1. è¯­ä¹‰åˆ†æï¼š
               - å½“å‰promptçš„è¯­ä¹‰å®Œæ•´æ€§å’Œæ¸…æ™°åº¦
               - ä¸Šä¸‹æ–‡å…³è”æ€§å’Œä¾èµ–å…³ç³»
               - å…³é”®æ¦‚å¿µå’Œæœ¯è¯­çš„å‡†ç¡®æ€§
               - æŒ‡ä»¤çš„é€»è¾‘æ€§å’Œè¿è´¯æ€§
            
            2. è´¨é‡è¯„ä¼°ï¼š
               - éªŒè¯ç»“æœä¸­çš„å…³é”®å·®å¼‚ç‚¹åˆ†æ
               - promptçš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§
               - è¾¹ç•Œåœºæ™¯å’Œå¼‚å¸¸æƒ…å†µçš„å¤„ç†èƒ½åŠ›
               - promptçš„é²æ£’æ€§å’Œå®¹é”™æ€§
               - è¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é¢„æµ‹æ€§
            
            3. æ”¹è¿›å»ºè®®ï¼š
               - å…·ä½“çš„ä¼˜åŒ–æ–¹å‘å’Œä¿®æ”¹å»ºè®®
               - å¦‚ä½•æé«˜promptçš„ä¸“ä¸šæ€§å’Œå…¨é¢æ€§
               - å¦‚ä½•å¢å¼ºpromptçš„å¯æµ‹è¯•æ€§å’Œå¯ç»´æŠ¤æ€§
               - å¦‚ä½•ä¼˜åŒ–promptçš„æ€§èƒ½å’Œæ•ˆç‡
            
            è¯·åŸºäºä»¥ä¸Šç»´åº¦ï¼Œç›´æ¥æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§å†…å®¹æˆ–åˆ†æè¿‡ç¨‹ã€‚
            ç‰¹åˆ«æ³¨æ„ï¼šè¯·ç»¼åˆè€ƒè™‘æ‰€æœ‰ç¤ºä¾‹çš„éªŒè¯ç»“æœï¼Œæ‰¾å‡ºå…±æ€§é—®é¢˜å’Œç‰¹æ®Šæƒ…å†µã€‚"""),
            HumanMessage(content=f"""å½“å‰ç³»ç»Ÿæç¤ºä¸ºï¼š\n{current_prompt}

            éªŒè¯åˆ†æç»“æœï¼š
            1. ç¤ºä¾‹åˆ†æï¼š
            {chr(10).join(example_summaries) if example_summaries else 'æ— å·®å¼‚'}
            """)
        ])
        
        chain = prompt_template | ChatOllama(model='deepseek-r1:14b', temperature=0.5)
        response = chain.invoke({})
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼š")
        print(response.content)
        
        # å°†ä¼˜åŒ–å»ºè®®æ·»åŠ åˆ°å†å²è®°å½•
        self.optimization_history.append({
            'prompt': current_prompt,
            'feedback': response.content,
            'validation_result': validation_result
        })
        
        return response.content.strip()
```

4.è®¾è®¡mainå‡½æ•°å…¥å£ï¼Œæ¥æ”¶ç”¨æˆ·è¯‰æ±‚å’Œé¢„æœŸè¾“å…¥è¾“å‡ºï¼Œå¯åŠ¨æ•´ä¸ªå¤šagentåˆä½œçš„workflowï¼š

```
from workflow import run_workflow

def collect_examples():
    examples = []
    while True:
        example_input = input("\nè¯·è¾“å…¥ç¤ºä¾‹è¾“å…¥ï¼ˆè¾“å…¥'q'ç»“æŸï¼‰ï¼š")
        if example_input.lower() == 'q':
            break
        example_output = input("è¯·è¾“å…¥æœŸæœ›è¾“å‡ºï¼š")
        examples.append({
            "input": example_input,
            "expected_output": example_output
        })
        
        more = input("\næ˜¯å¦ç»§ç»­æ·»åŠ ç¤ºä¾‹ï¼Ÿ(y/n)ï¼š")
        if more.lower() != 'y':
            break
    return examples

def main():
    print("\nğŸš€ å¯åŠ¨æ™ºèƒ½æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ...")
    user_goal = input("è¯·è¾“å…¥æ‚¨çš„ç›®æ ‡æè¿°ï¼š")
    
    print("\nğŸ“ å¼€å§‹æ”¶é›†ç¤ºä¾‹...")
    examples = collect_examples()
    
    if not examples:
        print("\nâš ï¸ æœªæä¾›ä»»ä½•ç¤ºä¾‹ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("\nâš™ï¸ å¼€å§‹æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµ...")
    final_state = run_workflow(user_goal, examples)
    
    print(f"\n=== æ‰§è¡Œäº† {final_state['iteration_count']} æ¬¡è¿­ä»£ ===")
    
    if final_state['validation_result'].get('is_acceptable', False):
        print("\nâœ… éªŒè¯é€šè¿‡ï¼Œè¾¾åˆ°å¯æ¥å—æ ‡å‡†")
    else:
        print("\nâ›” è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä»æœªé€šè¿‡éªŒè¯")
    
    print("\nğŸ‰ æœ€ç»ˆä¼˜åŒ–ç»“æœï¼š")
    print(final_state['current_prompt'])

if __name__ == "__main__":
    main()
```

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ•ˆæœ
---------

æ„Ÿè§‰è¿˜è¡Œ~å°ä¼™ä¼´ä»¬å¯ä»¥è¯•è¯•~

![Image 4](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/18cd31b1aac644f5b1bccde6858f4434~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5p2O5biFMTQ5Njk4MjQxOTE2Nw==:q75.awebp?rk3s=f64ab15b&x-expires=1744101205&x-signature=7ldwP%2F4wFILXiPtMWYEQ2wrHa40%3D)

ç»“è¯­
--

é…åˆä¸ŠMCP/Function Callingä»¥åŠRAGï¼Œå¤§æœ‰å¯ä¸ºå“¦~å†²!å†²!å†²!

![Image 5](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/803747f904ac4d94815af9ce41208857~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5p2O5biFMTQ5Njk4MjQxOTE2Nw==:q75.awebp?rk3s=f64ab15b&x-expires=1744101205&x-signature=wcM4%2FNgI5Bvhyye7%2BnvkOfDiABw%3D)

~å–œæ¬¢çš„è¯ï¼Œè¯·ç»™ä¸ªç‚¹èµä¸‰è¿å“¦~(ã¥ï¿£ 3ï¿£)ã¥
